{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, random, math, time\n",
    "import math\n",
    "import logging, warnings, csv, base64, gzip\n",
    "import json, datetime, numpy as np, torch, torch.nn as nn\n",
    "import torch.optim as optim, torch.nn.functional as F, torchaudio\n",
    "import torchaudio.transforms as transforms, torch.utils.checkpoint as checkpoint\n",
    "import torch.utils.tensorboard as tensorboard, torch.optim.lr_scheduler as lr_scheduler\n",
    "import transformers, neologdn, evaluate, MeCab, deepl, logging, datasets, tqdm, whisper\n",
    "import transformers.utils.logging\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Subset, IterableDataset\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "from torch.profiler import profile, ProfilerActivity, record_function\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import amp, Tensor\n",
    "from torch.optim import Adamax\n",
    "import logging\n",
    "from safetensors import safe_open\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from transformers.trainer_pt_utils import IterableDatasetShard\n",
    "from transformers.trainer_utils import is_main_process\n",
    "from transformers.trainer_pt_utils import find_batch_size, get_parameter_names\n",
    "from transformers import (\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    "    logging,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    PretrainedConfig,\n",
    "    GenerationConfig,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizerFast,\n",
    "    WhisperTokenizer,\n",
    "    WhisperModel,\n",
    "    WhisperConfig,\n",
    "    Adafactor,\n",
    "    TrainerCallback,\n",
    "    logging\n",
    ")\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "warnings.warn = lambda *args, **kwargs: None\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "checkpointing_args = {\"reentrant\": False}\n",
    "\n",
    "try:\n",
    "    from torch.nn.functional import scaled_dot_product_attention\n",
    "    SDPA_AVAILABLE = True\n",
    "except (ImportError, RuntimeError, OSError):\n",
    "    scaled_dot_product_attention = None\n",
    "    SDPA_AVAILABLE = False\n",
    "\n",
    "from whisper.decoding import decode as decode_function\n",
    "from whisper.decoding import detect_language as detect_language_function\n",
    "from whisper.transcribe import transcribe as transcribe_function\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "transformers.utils.logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasedCrossAttention(nn.Module):\n",
    "    def __init__(self, n_state, n_head, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.n_state = n_state\n",
    "        self.head_dim = n_state // n_head\n",
    "\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(n_head, 1, self.head_dim))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.norm = LayerNorm(n_state)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size, seq_length, _ = q.size()\n",
    "\n",
    "        q = self.query(q).view(batch_size, seq_length, self.n_head, self.head_dim)\n",
    "        k = self.key(k).view(batch_size, seq_length, self.n_head, self.head_dim)\n",
    "        v = self.value(v).view(batch_size, seq_length, self.n_head, self.head_dim)\n",
    "\n",
    "        qk = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5) + self.bias\n",
    "        if mask is not None:\n",
    "            qk = qk.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        w = F.softmax(qk, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "\n",
    "        out = (w @ v).transpose(1, 2).contiguous().view(batch_size, seq_length, -1)\n",
    "        out = self.norm(self.out(out) + q.view(batch_size, seq_length, -1))\n",
    "        return out\n",
    "\n",
    "class Conv1d(nn.Conv1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, nonlinearity='relu')\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def _conv_forward(self, x, weight, bias) -> Tensor:\n",
    "        weight = self.weight.to(x.dtype)\n",
    "        bias = None if self.bias is None else self.bias.to(x.dtype)\n",
    "        return super()._conv_forward(x, weight, bias)\n",
    "    \n",
    "class LearnedSinusoidalEmbeddings(nn.Module):\n",
    "    def __init__(self, n_ctx, n_state, checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_state = n_state\n",
    "        self.checkpointing = checkpointing\n",
    "\n",
    "        position = torch.arange(0, n_ctx, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, n_state, 2).float() * -(math.log(10000.0) / n_state))\n",
    "        features = torch.zeros(n_ctx, n_state)\n",
    "        features[:, 0::2] = torch.sin(position * div_term)\n",
    "        features[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('sinusoidal_features', features)\n",
    "\n",
    "        self.positional_embeddings = nn.Parameter(self.sinusoidal_features.clone())\n",
    "\n",
    "    def forward(self, positions):\n",
    "        if self.checkpointing:\n",
    "            position_embeddings = checkpoint(lambda x: self.positional_embeddings[x], positions)\n",
    "        else:\n",
    "            position_embeddings = self.positional_embeddings[positions]\n",
    "\n",
    "        position_embeddings = torch.nn.functional.normalize(position_embeddings, p=2, dim=-1)  \n",
    "        return position_embeddings\n",
    "\n",
    "class DynamicConvAttention(nn.Module):\n",
    "    def __init__(self, n_state, n_head, kernel_size=3, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.n_state = n_state\n",
    "        self.n_head = n_head\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.conv = nn.Conv1d(n_state, n_state, kernel_size, padding=kernel_size // 2, groups=n_head)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.out_proj = nn.Linear(n_state, n_state)\n",
    "\n",
    "        self.norm = LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        if embed_dim != self.n_state:\n",
    "            raise ValueError(f\"Expected embed_dim of {self.n_state}, but got {embed_dim}\")\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        conv_out = self.conv(x)\n",
    "        conv_out = conv_out.permute(0, 2, 1)\n",
    "        conv_out = self.norm(conv_out)\n",
    "        conv_out = self.dropout(conv_out)\n",
    "\n",
    "        attention_out = F.softmax(torch.matmul(q, k.transpose(-2, -1)) / (self.n_state ** 0.5), dim=-1)\n",
    "        attention_out = torch.matmul(attention_out, v)\n",
    "        \n",
    "        combined_out = conv_out + attention_out\n",
    "        combined_out = self.norm(combined_out)\n",
    "        \n",
    "        return self.out_proj(self.dropout(combined_out)) + x.permute(0, 2, 1)\n",
    "\n",
    "class HybridAttention(nn.Module):\n",
    "    def __init__(self, n_state, n_head, window_size=1, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.local_attn = nn.MultiheadAttention(n_state, n_head, dropout=dropout_rate)\n",
    "        self.global_attn = nn.MultiheadAttention(n_state, n_head, dropout=dropout_rate)\n",
    "        self.ln_local = LayerNorm(n_state)\n",
    "        self.ln_global = LayerNorm(n_state)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x_local = self.ln_local(x)\n",
    "        x_global = self.ln_global(x)\n",
    "        x_local = x_local.permute(1, 0, 2)\n",
    "        x_global = x_global.permute(1, 0, 2)\n",
    "        local_out = self.sliding_window_attention(x_local)\n",
    "        global_out, _ = self.global_attn(x_global, x_global, x_global)\n",
    "        combined_out = local_out + global_out\n",
    "        combined_out = combined_out.permute(1, 0, 2)\n",
    "        return self.dropout(combined_out)\n",
    "\n",
    "    def sliding_window_attention(self, x):\n",
    "        seq_len, batch_size, n_state = x.size()\n",
    "        window_size = min(self.window_size, max(1, seq_len // 4))\n",
    "        output = torch.zeros_like(x, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        for i in range(0, seq_len, window_size):\n",
    "            end = min(i + window_size, seq_len)\n",
    "            query = x[i:end, :, :]\n",
    "            start = max(0, i - window_size)\n",
    "            key = x[start:end, :, :]\n",
    "            value = x[start:end, :, :]\n",
    "            attn_output, _ = self.local_attn(query, key, value)\n",
    "            output[i:end, :, :] = attn_output[:end - i, :, :]\n",
    "\n",
    "        return output\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        x = (x - mean) / (std + self.eps)\n",
    "        return self.gamma * x + self.beta\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, dropout_rate: float = 0.01, use_batchnorm: bool = True, activation: str = 'relu'):\n",
    "        super(Linear, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.activation = activation\n",
    "\n",
    "        if self.use_batchnorm:\n",
    "            self.batchnorm = nn.BatchNorm1d(out_features)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.linear.weight, nonlinearity=self.activation)\n",
    "        if self.linear.bias is not None:\n",
    "            nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = x.view(-1, x.size(-1))  \n",
    "        x = self.linear(x)\n",
    "\n",
    "        if self.use_batchnorm:\n",
    "            x = self.batchnorm(x)\n",
    "\n",
    "        x = self.apply_activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(batch_size, seq_len, -1)  \n",
    "        \n",
    "        return x\n",
    "\n",
    "    def apply_activation(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif self.activation == 'tanh':\n",
    "            return torch.tanh(x)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported activation function: {self.activation}')\n",
    "\n",
    "def sinusoids(length, channels, max_timescale=10000):\n",
    "    \"\"\"Returns sinusoids for positional embedding\"\"\"\n",
    "    assert channels % 2 == 0\n",
    "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
    "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    use_sdpa = True\n",
    "\n",
    "    def __init__(self, n_state: int, n_head: int, base: int = 10000, checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "        self.h_dim = n_state // n_head\n",
    "        self.checkpointing=checkpointing\n",
    "\n",
    "        self.rotation_matrix = nn.Parameter(torch.empty(self.h_dim, self.h_dim))\n",
    "        nn.init.orthogonal_(self.rotation_matrix)\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.h_dim, 2).float() / self.h_dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def rotate_queries_or_keys(self, x):\n",
    "        sinusoid_inp = torch.einsum('i , j -> i j', torch.arange(x.shape[1], device=x.device), self.inv_freq)\n",
    "        sin = sinusoid_inp.sin()[None, :, None, :]\n",
    "        cos = sinusoid_inp.cos()[None, :, None, :]\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        x = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor, xa: Optional[torch.Tensor] = None, mask: Optional[torch.Tensor] = None, kv_cache: Optional[dict] = None):\n",
    "        q = self.query(x)\n",
    "\n",
    "        if kv_cache is None or xa is None or self.key not in kv_cache:\n",
    "            k = self.key(x if xa is None else xa)\n",
    "            v = self.value(x if xa is None else xa)\n",
    "        else:\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "\n",
    "        q = q.view(q.shape[0], q.shape[1], self.n_head, -1)\n",
    "        k = k.view(k.shape[0], k.shape[1], self.n_head, -1)\n",
    "\n",
    "        q = self.rotate_queries_or_keys(q)\n",
    "        k = self.rotate_queries_or_keys(k)\n",
    "        q = torch.matmul(q, self.rotation_matrix)\n",
    "        k = torch.matmul(k, self.rotation_matrix)\n",
    "\n",
    "        q = q.view(q.shape[0], q.shape[1], -1)\n",
    "        k = k.view(k.shape[0], k.shape[1], -1)\n",
    "\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "        return self.out(wv), qk\n",
    "\n",
    "    def qkv_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        n_batch, n_ctx, n_state = q.shape\n",
    "        scale = (n_state // self.n_head) ** -0.25\n",
    "        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        if SDPA_AVAILABLE and MultiHeadAttention.use_sdpa:\n",
    "            a = scaled_dot_product_attention(q, k, v, is_causal=mask is not None and n_ctx > 1)\n",
    "            out = a.permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = None\n",
    "        else:\n",
    "            qk = (q * scale) @ (k * scale).transpose(-1, -2)\n",
    "            if mask is not None:\n",
    "                qk = qk + mask[:n_ctx, :n_ctx]\n",
    "            qk = qk.float()\n",
    "\n",
    "            w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "            out = (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = qk.detach()\n",
    "\n",
    "        return out, qk\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False, checkpointing=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(n_state, n_head)\n",
    "        self.attn_ln = LayerNorm(n_state)\n",
    "        self.checkpointing=checkpointing\n",
    "        \n",
    "        self.cross_attn = (\n",
    "            MultiHeadAttention(n_state, n_head) if cross_attention else None\n",
    "        )\n",
    "        self.cross_attn_ln = LayerNorm(n_state) if cross_attention else None\n",
    "\n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp = nn.Sequential(\n",
    "            Linear(n_state, n_mlp), nn.GELU(), Linear(n_mlp, n_state)\n",
    "        )\n",
    "        self.mlp_ln = LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, xa: Optional[torch.Tensor] = None, mask: Optional[torch.Tensor] = None, kv_cache: Optional[dict] = None):\n",
    "        residual = x\n",
    "        x = self.attn_ln(x)\n",
    "        x = residual + self.attn(x, mask=mask, kv_cache=kv_cache)[0]\n",
    "\n",
    "        if self.cross_attn:\n",
    "            residual = x\n",
    "            x = self.cross_attn_ln(x)\n",
    "            x = residual + self.cross_attn(x, xa, kv_cache=kv_cache)[0]\n",
    "\n",
    "        residual = x\n",
    "        x = self.mlp_ln(x)\n",
    "        x = residual + self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class RotaryEmbeddingWithRotation(nn.Module):\n",
    "    def __init__(self, n_state, n_head, base=10000):\n",
    "        super().__init__()\n",
    "        self.n_state = n_state\n",
    "        self.n_head = n_head\n",
    "        self.h_dim = n_state // n_head\n",
    "        \n",
    "        self.rotation_matrix = nn.Parameter(torch.eye(self.h_dim))\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, self.h_dim, 2).float() / self.h_dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.orthogonal_(self.rotation_matrix)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 3:\n",
    "            batch_size, seq_len, n_state = x.size()\n",
    "        elif x.dim() == 4:\n",
    "            batch_size, seq_len, n_head, h_dim = x.size()\n",
    "            n_state = n_head * h_dim\n",
    "            x = x.view(batch_size, seq_len, n_state)\n",
    "        else:\n",
    "            raise ValueError(f\"Expected input tensor to be 3D or 4D, but got {x.dim()}D\")\n",
    "\n",
    "        if n_state != self.n_state:\n",
    "            raise ValueError(f\"Expected n_state of {self.n_state}, but got {n_state}\")\n",
    "\n",
    "        x = x.reshape(batch_size, seq_len, self.n_head, self.h_dim)\n",
    "\n",
    "        x = x.reshape(-1, self.h_dim)\n",
    "        rotated_x = torch.matmul(x, self.rotation_matrix)\n",
    "        rotated_x = rotated_x.reshape(batch_size, seq_len, self.n_head, self.h_dim)\n",
    "\n",
    "        sinusoid_inp = torch.einsum('i, j -> i j', torch.arange(seq_len, device=x.device), self.inv_freq)\n",
    "        sin = sinusoid_inp.sin()[None, :, None, :]\n",
    "        cos = sinusoid_inp.cos()[None, :, None, :]\n",
    "        x1, x2 = rotated_x[..., ::2], rotated_x[..., 1::2]\n",
    "        rotated_x = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "        \n",
    "        rotated_x = rotated_x.reshape(batch_size, seq_len, self.n_state)\n",
    "        return rotated_x\n",
    "\n",
    "class LearnedSinusoidalEmbeddings(nn.Module):\n",
    "    def __init__(self, n_ctx, n_state, checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_state = n_state\n",
    "        self.checkpointing = checkpointing\n",
    "\n",
    "        position = torch.arange(0, n_ctx, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, n_state, 2).float() * -(math.log(10000.0) / n_state))\n",
    "        features = torch.zeros(n_ctx, n_state)\n",
    "        features[:, 0::2] = torch.sin(position * div_term)\n",
    "        features[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('sinusoidal_features', features)\n",
    "\n",
    "        self.positional_embeddings = nn.Parameter(self.sinusoidal_features.clone())\n",
    "\n",
    "    def forward(self, positions):\n",
    "        if self.checkpointing:\n",
    "            position_embeddings = checkpoint(lambda x: self.positional_embeddings[x], positions)\n",
    "        else:\n",
    "            position_embeddings = self.positional_embeddings[positions]\n",
    "\n",
    "        position_embeddings = torch.nn.functional.normalize(position_embeddings, p=2, dim=-1)  \n",
    "        return position_embeddings\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int, checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
    "        self.positional_embedding = LearnedSinusoidalEmbeddings(n_ctx, n_state, checkpointing=checkpointing)\n",
    "        self.rotor_layer = RotaryEmbeddingWithRotation(n_state, n_head)\n",
    "        self.checkpointing = checkpointing\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [ResidualAttentionBlock(n_state, n_head, checkpointing=checkpointing) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_post = LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = F.gelu(self.conv1(x))\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.rotor_layer(x)\n",
    "        pos_emb = self.positional_embedding(torch.arange(x.size(1), device=x.device)).unsqueeze(0)\n",
    "        x = x + pos_emb\n",
    "\n",
    "        for block in self.blocks:\n",
    "            if self.checkpointing:\n",
    "                x = checkpoint(block, x)\n",
    "            else:\n",
    "                x = block(x)\n",
    "        x = self.ln_post(x)\n",
    "        return x\n",
    "\n",
    "class TextDecoder(nn.Module):\n",
    "    def __init__(self, n_vocab, n_ctx, n_state, n_head, n_layer, checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_state)\n",
    "        self.positional_embedding = LearnedSinusoidalEmbeddings(n_ctx, n_state, checkpointing=checkpointing)\n",
    "        self.rotor_layer = RotaryEmbeddingWithRotation(n_state, n_head)\n",
    "        self.checkpointing = checkpointing\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResidualAttentionBlock(n_state, n_head, cross_attention=True, checkpointing=checkpointing)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln = LayerNorm(n_state)\n",
    "        mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)\n",
    "        self.register_buffer(\"mask\", mask, persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, xa: torch.Tensor, kv_cache: Optional[dict] = None):\n",
    "        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n",
    "        positions = torch.arange(x.shape[1], device=x.device) + offset\n",
    "        pos_emb = self.positional_embedding(positions).unsqueeze(0)\n",
    "\n",
    "        x = self.token_embedding(x) + pos_emb\n",
    "        x = x.to(xa.dtype)\n",
    "\n",
    "        batch_size, seq_length, embedding_dim = x.shape\n",
    "        num_heads = self.n_head\n",
    "        head_dim = embedding_dim // num_heads\n",
    "        x = x.view(batch_size, seq_length, num_heads, head_dim)\n",
    "\n",
    "        x = self.rotor_layer(x)\n",
    "        x = x.view(batch_size, seq_length, embedding_dim)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            if self.checkpointing:\n",
    "                x = checkpoint(block, x, xa, self.mask, kv_cache)\n",
    "            else:\n",
    "                x = block(x, xa, self.mask, kv_cache)\n",
    "\n",
    "        x = self.ln(x)\n",
    "        logits = (x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)).float()\n",
    "\n",
    "        return logits\n",
    "\n",
    "class Whisper(nn.Module):\n",
    "    def __init__(self, config: WhisperConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.encoder = AudioEncoder(\n",
    "            self.config.n_mels,\n",
    "            self.config.n_audio_ctx,\n",
    "            self.config.n_audio_state,\n",
    "            self.config.n_audio_head,\n",
    "            self.config.n_audio_layer,\n",
    "            self.config.checkpointing,\n",
    "        )\n",
    "        self.decoder = TextDecoder(\n",
    "            self.config.n_vocab,\n",
    "            self.config.n_text_ctx,\n",
    "            self.config.n_text_state,\n",
    "            self.config.n_text_head,\n",
    "            self.config.n_text_layer,\n",
    "            self.config.checkpointing,\n",
    "        )\n",
    "\n",
    "        all_heads = torch.zeros(\n",
    "            self.config.n_text_layer, self.config.n_text_head, dtype=torch.bool\n",
    "        )\n",
    "        all_heads[self.config.n_text_layer // 2 :] = True\n",
    "        self.register_buffer(\"alignment_heads\", all_heads.to_sparse(), persistent=False)\n",
    "\n",
    "    def set_alignment_heads(self, dump: bytes):\n",
    "        array = np.frombuffer(\n",
    "            gzip.decompress(base64.b85decode(dump)), dtype=bool\n",
    "        ).copy()\n",
    "        mask = torch.from_numpy(array).reshape(\n",
    "            self.config.n_text_layer, self.config.n_text_head\n",
    "        )\n",
    "        self.register_buffer(\"alignment_heads\", mask.to_sparse(), persistent=False)\n",
    "\n",
    "    def embed_audio(self, mel: torch.Tensor):\n",
    "        return self.encoder(mel)\n",
    "\n",
    "    def logits(self, tokens: torch.Tensor, input_features: torch.Tensor):\n",
    "        return self.decoder(tokens, input_features)\n",
    "    \n",
    "    @staticmethod\n",
    "    def shift_tokens_right(input_ids: torch.Tensor, pad_token_id, decoder_start_token_id) -> torch.Tensor:\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape, dtype=torch.long)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "        shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\"pad_token_id has to be defined.\")\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "        return shifted_input_ids\n",
    "\n",
    "    def forward(self, input_features: torch.Tensor, labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
    "        input_features = input_features.float()\n",
    "        if torch.isnan(input_features).any():\n",
    "            print(\"NaNs detected in input features\")\n",
    "\n",
    "        encoded_features = self.encoder(input_features)\n",
    "        if torch.isnan(encoded_features).any():\n",
    "            print(\"NaNs detected in encoded features\")\n",
    "\n",
    "        if labels is not None:\n",
    "            labels = labels.long()\n",
    "            decoder_input_ids = self.shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
    "            if torch.isnan(decoder_input_ids).any():\n",
    "                print(\"NaNs detected in decoder input IDs\")\n",
    "        else:\n",
    "            decoder_input_ids = None\n",
    "\n",
    "        logits = self.decoder(decoder_input_ids, encoded_features)\n",
    "        if torch.isnan(logits).any():\n",
    "            print(\"NaNs detected in logits\")\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            logits = logits.view(-1, self.config.n_vocab)\n",
    "            labels = labels.view(-1).long()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            if torch.isnan(loss).any():\n",
    "                print(\"NaNs detected in loss- do the NaN dance!\")\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits, \"input_features\": encoded_features}\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @property\n",
    "    def is_multilingual(self):\n",
    "        return self.config.n_vocab >= 51865\n",
    "\n",
    "    @property\n",
    "    def num_languages(self):\n",
    "        return self.config.n_vocab - 51765 - int(self.is_multilingual)\n",
    "\n",
    "    def install_kv_cache_hooks(self, cache: Optional[dict] = None):\n",
    "        cache = {**cache} if cache is not None else {}\n",
    "        hooks = []\n",
    "\n",
    "        def save_to_cache(module, _, output):\n",
    "            if module not in cache or output.shape[1] > self.config.n_text_ctx:\n",
    "                cache[module] = output\n",
    "            else:\n",
    "                cache[module] = torch.cat([cache[module], output], dim=1).detach()\n",
    "            return cache[module]\n",
    "\n",
    "        def install_hooks(layer: nn.Module):\n",
    "            if isinstance(layer, MultiHeadAttention):\n",
    "                hooks.append(layer.key.register_forward_hook(save_to_cache))\n",
    "                hooks.append(layer.value.register_forward_hook(save_to_cache))\n",
    "\n",
    "        self.decoder.apply(install_hooks)\n",
    "        return cache, hooks\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings: torch.nn.Embedding):\n",
    "        self.decoder.token_embedding = new_embeddings\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.decoder.token_embedding\n",
    "\n",
    "    def resize_token_embeddings(self, new_num_tokens: int):\n",
    "        old_embeddings = self.get_input_embeddings()\n",
    "        old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n",
    "        new_embeddings = torch.nn.Embedding(new_num_tokens, old_embedding_dim)\n",
    "    \n",
    "        new_embeddings.weight.data[:old_num_tokens, :] = old_embeddings.weight.data\n",
    "        self.set_input_embeddings(new_embeddings)\n",
    "        self.config.n_vocab = new_num_tokens\n",
    "\n",
    "    detect_language = detect_language_function\n",
    "    transcribe = transcribe_function\n",
    "    decode = decode_function\n",
    "\n",
    "    def save_pretrained(self, save_directory, safetensor=False):\n",
    "        self.config.save_pretrained(save_directory)\n",
    "        if safetensor:\n",
    "            safetensor_path = os.path.join(save_directory, \"model.safetensors\")\n",
    "            with safe_open(safetensor_path, framework=\"pt\", mode=\"w\") as f:\n",
    "                for key, value in self.state_dict().items():\n",
    "                    f.set_tensor(key, value)\n",
    "        else:\n",
    "            torch.save(self.state_dict(), os.path.join(save_directory, \"pytorch_model.bin\"))\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, safetensor=False, *model_args, **kwargs):\n",
    "        config = WhisperConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "        model = cls(config, *model_args, **kwargs)\n",
    "        if safetensor:\n",
    "            safetensor_path = f\"{pretrained_model_name_or_path}/model.safetensors\"\n",
    "            with safe_open(safetensor_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "                state_dict = {key: torch.tensor(f.get_tensor(key)) for key in f.keys()}\n",
    "        else:\n",
    "            state_dict = torch.load(os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin\"), map_location=\"cpu\")\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        return model\n",
    "\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        return {'input_features': input_ids}\n",
    "\n",
    "    def _prepare_decoder_input_ids_for_generation(self, batch_size, decoder_start_token_id=None, bos_token_id=None):\n",
    "        return torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * self.config.decoder_start_token_id\n",
    "\n",
    "    def can_generate(self):\n",
    "        return True\n",
    "    \n",
    "    def generate(self, inputs, **kwargs):\n",
    "        encoder_outputs = self.encoder(inputs)\n",
    "        decoder_input_ids = torch.zeros((inputs.size(0), 1), dtype=torch.long, device=inputs.device)\n",
    "        outputs = self.decoder(decoder_input_ids, encoder_outputs)\n",
    "        return outputs.argmax(dim=-1)\n",
    "    \n",
    "    def generate_beam_search(self, inputs, **kwargs):\n",
    "        encoder_outputs = self.encoder(inputs)\n",
    "        decoder_input_ids = torch.zeros((inputs.size(0), 1), dtype=torch.long, device=inputs.device)\n",
    "        outputs = self.decoder(decoder_input_ids, encoder_outputs)\n",
    "        return outputs.argmax(dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wave(wave_path, sample_rate: int = 16000) -> torch.Tensor:\n",
    "    waveform, sr = torchaudio.load(wave_path, normalize=True)\n",
    "    if sample_rate != sr:\n",
    "        waveform = torchaudio.transforms.Resample(sr, sample_rate)(waveform)\n",
    "    return waveform\n",
    "\n",
    "class audioDataset(Dataset):\n",
    "    def __init__(self, csv_file, aud_dir, tokenizer, sample_rate=16000):\n",
    "        self.aud_dir = aud_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_rate = sample_rate\n",
    "        self.samples = []\n",
    "\n",
    "        with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader) \n",
    "            for row in reader:\n",
    "                aud_path, label = row[0], row[1]\n",
    "                self.samples.append((aud_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        aud_path, label = self.samples[idx]\n",
    "        label = handle_unknown_characters(label)\n",
    "        aud = f'{self.aud_dir}/{aud_path}'\n",
    "        return {\n",
    "            'input_features': aud,\n",
    "            'labels': label,\n",
    "            'input_ids': label \n",
    "        }\n",
    "\n",
    "def handle_unknown_characters(label): \n",
    "    label = label.encode('utf-8').decode('utf-8', errors='replace')\n",
    "    label = neologdn.normalize(label, repeat=1)\n",
    "    return label\n",
    "\n",
    "class WhisperDataCollatorWithPadding:\n",
    "    def __init__(self, tokenizer, n_mels, n_fft, hop_length, sample_rate=16000):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.mel_spectrogram_transform = transforms.MelSpectrogram(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_fft=self.n_fft,\n",
    "            n_mels=self.n_mels,\n",
    "            hop_length=self.hop_length\n",
    "        )\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_features, dec_input_ids, labels = [], [], []\n",
    "\n",
    "        for f in features:\n",
    "            aud_path = f['input_features']\n",
    "            aud, _ = torchaudio.load(aud_path, normalize=True)\n",
    "            aud = whisper.pad_or_trim(aud.flatten())\n",
    "\n",
    "            mel_spectrogram = self.mel_spectrogram_transform(aud)\n",
    "            log_mel_spectrogram = torch.log(mel_spectrogram + 1e-8)  \n",
    "\n",
    "            label = handle_unknown_characters(f['labels']) \n",
    "            encoded_input = self.tokenizer.encode(label)\n",
    "            encoded_label = self.tokenizer.encode(label)\n",
    "\n",
    "            dec_input_ids.append([self.tokenizer.bos_token_id] + encoded_input)\n",
    "            labels.append(encoded_label + [self.tokenizer.eos_token_id])\n",
    "            input_features.append(log_mel_spectrogram)\n",
    "\n",
    "        input_features = torch.stack(input_features)\n",
    "\n",
    "        input_lengths = [len(ids) for ids in dec_input_ids]\n",
    "        label_lengths = [len(lab) for lab in labels]\n",
    "        max_len = max(input_lengths + label_lengths)\n",
    "\n",
    "        dec_input_ids = [np.pad(ids, (0, max_len - len(ids)), 'constant', constant_values=self.tokenizer.pad_token_id) for ids in dec_input_ids]\n",
    "        labels = [np.pad(lab, (0, max_len - len(lab)), 'constant', constant_values=-100) for lab in labels]\n",
    "\n",
    "        batch = {\n",
    "            \"input_ids\": dec_input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"input_features\": input_features\n",
    "        }\n",
    "        batch = {k: torch.tensor(v, requires_grad=False) for k, v in batch.items()}\n",
    "        return batch\n",
    "\n",
    "metrics_cer = evaluate.load(\"cer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred[\"predictions\"]\n",
    "    label_ids = pred[\"label_ids\"]\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    cer = 100 * metrics_cer.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"cer\": cer}\n",
    "\n",
    "checkpoint_dir = 'D:/proj3/checkpoints/'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "log_dir = \"D:/proj3/logs/run_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "writer = SummaryWriter(log_dir)\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(log_dir, 'training.log'), \n",
    "    filemode='w', \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate(model, train_loader, eval_loader, optimizer, scheduler, loss_fn, num_epochs=1, max_steps=None, device='cuda', accumulation_steps=1, clear_cache=True, log_interval=10, eval_interval=20, save_interval=100, checkpoint_dir=\"checkpoint_dir\", log_dir=\"log_dir\"):\n",
    "    model.to(device)\n",
    "    global_step = 0\n",
    "    scaler = GradScaler()\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    lr_warning_printed = False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if max_steps is not None and global_step >= max_steps:\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            if max_steps is not None and global_step >= max_steps:\n",
    "                break\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            input_features = batch['input_features'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].long().to(device)\n",
    "\n",
    "            with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "                with record_function(\"model_training\"):\n",
    "                    with torch.amp.autocast(device_type='cuda'):\n",
    "                        input_features_encoded = model.encoder(input_features)\n",
    "                        decoder_output = model.decoder(input_ids, input_features_encoded)\n",
    "                        logits = decoder_output.view(-1, decoder_output.size(-1))\n",
    "                        loss = loss_fn(logits, labels.view(-1))\n",
    "                        total_loss += loss.item()\n",
    "                        loss = loss / accumulation_steps\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "\n",
    "                    if (step + 1) % accumulation_steps == 0:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        if clear_cache:\n",
    "                            torch.cuda.empty_cache()\n",
    "\n",
    "            global_step += 1\n",
    "            end_time = time.time()\n",
    "            samples_per_sec = len(batch['input_features']) / (end_time - start_time)\n",
    "\n",
    "            total_norm = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** (1. / 2)\n",
    "\n",
    "            if global_step % log_interval == 0:\n",
    "                writer.add_scalar('Loss/train', total_loss / (step + 1), global_step)\n",
    "                writer.add_scalar('GradientNorm', total_norm, global_step)\n",
    "                \n",
    "                lr = optimizer.param_groups[0].get('lr', None)\n",
    "                if lr is not None:\n",
    "                    writer.add_scalar('LearningRate', lr, global_step)\n",
    "                else:\n",
    "                    if not lr_warning_printed:\n",
    "                        print(f\"Warning: Learning rate is None at step {global_step}\")\n",
    "                        lr_warning_printed = True\n",
    "\n",
    "                writer.add_scalar('SamplesPerSec', samples_per_sec, global_step)\n",
    "\n",
    "            if global_step % eval_interval == 0:\n",
    "                model.eval()\n",
    "                eval_loss = 0\n",
    "                all_predictions = []\n",
    "                all_labels = []\n",
    "                with torch.no_grad():\n",
    "                    for eval_batch in eval_loader:\n",
    "                        input_features = eval_batch['input_features'].to(device)\n",
    "                        input_ids = eval_batch['input_ids'].to(device)\n",
    "                        labels = eval_batch['labels'].long().to(device)\n",
    "                        input_features_encoded = model.encoder(input_features)\n",
    "                        decoder_output = model.decoder(input_ids, input_features_encoded)\n",
    "                        logits = decoder_output.view(-1, decoder_output.size(-1))\n",
    "                        loss = loss_fn(logits, labels.view(-1))\n",
    "                        eval_loss += loss.item()\n",
    "                        all_predictions.extend(torch.argmax(decoder_output, dim=-1).cpu().numpy().tolist())\n",
    "                        all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "                eval_loss /= len(eval_loader)\n",
    "                predictions = {\"predictions\": np.array(all_predictions, dtype=\"object\"), \"label_ids\": np.array(all_labels, dtype=\"object\")}\n",
    "                metrics = compute_metrics(predictions)\n",
    "                writer.add_scalar('Loss/eval', eval_loss, global_step)\n",
    "                writer.add_scalar('CER', metrics['cer'], global_step)\n",
    "                scheduler.step()  # Step the scheduler\n",
    "\n",
    "                sample_indices = range(min(1, len(all_predictions))) \n",
    "                for idx in sample_indices:\n",
    "                    pred_str = tokenizer.decode(all_predictions[idx], skip_special_tokens=True)\n",
    "                    label_str = tokenizer.decode(all_labels[idx], skip_special_tokens=True)\n",
    "                    print(f\"Evaluation Loss: {eval_loss:.4f}\")\n",
    "                    print(f\"Evaluation Sample {idx}: Prediction: {pred_str}, Label: {label_str}\")\n",
    "                    logging.info(f\"Evaluation Sample {idx}: Prediction: {pred_str}, Label: {label_str}\")\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                print(f\"Evaluation Loss: {eval_loss:.4f}\")\n",
    "                print(f\"Character Error Rate (CER): {metrics['cer']:.4f}\")\n",
    "\n",
    "            if global_step % save_interval == 0:\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_step_{global_step}.pt')\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"Model saved at step {global_step} to {checkpoint_path}\")\n",
    "                logging.info(f\"Model saved at step {global_step} to {checkpoint_path}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}')\n",
    "        logging.info(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "    final_model_path = os.path.join(checkpoint_dir, 'final_model.pt')\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "    logging.info(f\"Final model saved to {final_model_path}\")\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    tokenizer = WhisperTokenizerFast.from_pretrained(\"openai/whisper-small\")\n",
    "    csv_file = 'D:/proj/datasets/gf_1/metadata.csv'\n",
    "    audio_dir = 'D:/proj/datasets/gf_1/'\n",
    "\n",
    "    def train_val_dataset(dataset, val_split=0.001):\n",
    "        train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "        datasets = {}\n",
    "        datasets['train'] = Subset(dataset, train_idx)\n",
    "        datasets['val'] = Subset(dataset, val_idx)\n",
    "        return datasets\n",
    "\n",
    "    dataset = audioDataset(csv_file, audio_dir, tokenizer)\n",
    "    datasets = train_val_dataset(dataset)\n",
    "    train_dataset = datasets['train']\n",
    "    eval_dataset = datasets['val']\n",
    "    \n",
    "    def train_dataloader():   \n",
    "        return DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=1,\n",
    "            drop_last=False, \n",
    "            shuffle=True, \n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "    def eval_dataloader():\n",
    "        return DataLoader(\n",
    "            eval_dataset,\n",
    "            batch_size=1, \n",
    "            drop_last=True,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "    collate_fn = WhisperDataCollatorWithPadding(tokenizer, n_fft=1024, hop_length=256, n_mels=80)\n",
    "    train_loader = train_dataloader()\n",
    "    eval_loader = eval_dataloader()\n",
    "\n",
    "    config = WhisperConfig(\n",
    "        n_mels=80,\n",
    "        n_audio_ctx=1500,\n",
    "        n_audio_state=1024,\n",
    "        n_audio_head=16,\n",
    "        n_audio_layer=24,\n",
    "        n_vocab=51865,\n",
    "        n_text_ctx=448,\n",
    "        n_text_state=1024,\n",
    "        n_text_head=16,\n",
    "        n_text_layer=20,\n",
    "        checkpointing=True,\n",
    "        )\n",
    "\n",
    "    model = Whisper(config).cuda()\n",
    "    # model.resize_token_embeddings(len(tokenizer))\n",
    "    optimizer = transformers.Adafactor(model.parameters(), \n",
    "                                    clip_threshold=0.99, \n",
    "                                    weight_decay=0.025, \n",
    "                                    scale_parameter=True, \n",
    "                                    relative_step=False, \n",
    "                                    warmup_init=False, \n",
    "                                    lr=2.25e-3)\n",
    "\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "    train_and_evaluate(model, train_loader, eval_loader, optimizer, scheduler, loss_fn, max_steps=100, num_epochs=1, device='cuda', accumulation_steps=1, clear_cache=True, log_interval=1, eval_interval=10, save_interval=100, checkpoint_dir=checkpoint_dir, log_dir=log_dir)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
