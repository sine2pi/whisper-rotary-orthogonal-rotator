{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is expermental code for the integration of different attention mechanisms in a transformer whisper-like model. \n",
    "# Although everything runs and works the code is not complete and is meant to be used as a reference for further development.\n",
    "# The idea is to create a transformer model that can learn from different attention mechanisms and combine them to improve performance.\n",
    "# This model includes the following attention mechanisms:\n",
    "# 1. Dynamic Convolutional Attention\n",
    "# 2. Hybrid Attention\n",
    "# 3. Biased Attention\n",
    "# 4. Augmented Memory\n",
    "# 5. Rotary and Learned Sinusoidal Embeddings\n",
    "# Other components like LayerNorm, GroupNorm(RMS), custom linear and conv1 blocks and MLP are also included to complete the transformer block.\n",
    "# The code is written in PyTorch and is meant to be used as a reference for further development and experimentation. Training loop and data processing are not included in pytorch with optional adaptation for hugging face trainer and datasets.\n",
    "# The code is not optimized and is meant for educational purposes only.\n",
    "\n",
    "# The goal is to integrate these mechanisms into a single transformer block that can leverage the strengths of each attention mechanism to improve performance on various tasks.   \n",
    "\n",
    "# 1. Dynamic Convolutional Attention and Hybrid Attention\n",
    "# Natural Synergy: Dynamic convolutional attention can adapt convolutional filters based on the input, providing a local context that hybrid attention can leverage. Combining these can enhance both global and local context understanding.\n",
    "\n",
    "# Integration Idea: Use dynamic convolutional attention within the local convolution part of the hybrid attention mechanism. This way, the dynamic adjustments made by the convolutional attention can be directly utilized by the hybrid attention layers.\n",
    "\n",
    "# 2. Biased Attention and Augmented Memory\n",
    "# Natural Synergy: Biased attention can prioritize important features, while augmented memory can store and retrieve long-term dependencies. Together, they can ensure that important features are not only highlighted but also remembered over long sequences.\n",
    "\n",
    "# Integration Idea: Embed bias terms in the augmented memory retrieval process, allowing the model to focus on and recall important features over extended periods.\n",
    "\n",
    "# 3. Rotary and Learned Sinusoidal Embeddings with Hybrid Attention\n",
    "# Natural Synergy: Rotary and learned sinusoidal embeddings enhance positional encoding, which can be crucial for hybrid attention mechanisms that need to maintain the order of information while attending to both local and global contexts.\n",
    "\n",
    "# Integration Idea: Apply rotary and learned sinusoidal embeddings within the hybrid attention layers to improve positional awareness and ensure the model accurately captures the order and structure of the input data.\n",
    "\n",
    "# Example Implementation of Integrated Block\n",
    "# Here's an idea of how one might begin to integrate these components into a new block:\n",
    "\n",
    "# python\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class IntegratedAttentionBlock(nn.Module):\n",
    "#     def __init__(self, n_state: int, n_head: int, window_size: int = 5, dropout_rate=0.1, use_GroupNorm=False):\n",
    "#         super().__init__()\n",
    "#         self.dynamic_conv_attn = DynamicConvAttention(n_state, n_head, dropout_rate=dropout_rate)\n",
    "#         self.hybrid_attention = HybridAttention(n_state, n_head, window_size=window_size, dropout_rate=dropout_rate, use_GroupNorm=use_GroupNorm)\n",
    "#         self.biased_attention = BiasedAttention(n_state, n_head, dropout_rate=dropout_rate)\n",
    "#         self.augmented_memory = AugmentedMemory(n_state, memory_size=512, n_head=n_head, dropout_rate=dropout_rate)\n",
    "\n",
    "#         self.attn_ln = GroupNorm(num_groups=4, num_channels=n_state) if use_GroupNorm else LayerNorm(n_state)\n",
    "#         n_mlp = n_state * 4\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(n_state, n_mlp),\n",
    "#             GroupNorm(num_groups=4, num_channels=n_mlp) if use_GroupNorm else LayerNorm(n_mlp),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(p=dropout_rate),\n",
    "#             nn.Linear(n_mlp, n_state)\n",
    "#         )\n",
    "#         self.mlp_ln = GroupNorm(num_groups=4, num_channels=n_state) if use_GroupNorm else LayerNorm(n_state)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor):\n",
    "#         # Dynamic Convolutional Attention\n",
    "#         x = self.dynamic_conv_attn(x)\n",
    "#         # Hybrid Attention with Rotary and Learned Sinusoidal Embeddings\n",
    "#         x = self.hybrid_attention(x)\n",
    "#         # Biased Attention within Augmented Memory\n",
    "#         x = self.augmented_memory(x)\n",
    "\n",
    "#         x = self.attn_ln(x)\n",
    "#         mlp_input = self.mlp_ln(x)\n",
    "#         x = x + self.mlp(mlp_input)\n",
    "\n",
    "#         return x\n",
    "\n",
    "# class DynamicConvAttention(nn.Module):\n",
    "#     def __init__(self, n_state, n_head, dropout_rate=0.1):\n",
    "#         super(DynamicConvAttention, self).__init__()\n",
    "#         # Define the dynamic convolutional attention components\n",
    "#         pass\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Implement the dynamic convolutional attention logic\n",
    "#         return x\n",
    "\n",
    "# class HybridAttention(nn.Module):\n",
    "#     def __init__(self, n_state, n_head, window_size=5, dropout_rate=0.1, use_GroupNorm=False):\n",
    "#         super(HybridAttention, self).__init__()\n",
    "#         # Define the hybrid attention components with rotary and learned sinusoidal embeddings\n",
    "#         pass\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Implement the hybrid attention logic\n",
    "#         return x\n",
    "\n",
    "# class BiasedAttention(nn.Module):\n",
    "#     def __init__(self, n_state, n_head, dropout_rate=0.1):\n",
    "#         super(BiasedAttention, self).__init__()\n",
    "#         # Define the biased attention components\n",
    "#         pass\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Implement the biased attention logic\n",
    "#         return x\n",
    "\n",
    "# class AugmentedMemory(nn.Module):\n",
    "#     def __init__(self, n_state, memory_size, n_head, dropout_rate=0.1):\n",
    "#         super(AugmentedMemory, self).__init__()\n",
    "#         # Define the augmented memory components\n",
    "#         pass\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Implement the augmented memory logic\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, random, math, time\n",
    "import math\n",
    "import logging, warnings, csv, base64, gzip\n",
    "import json, datetime, numpy as np, torch, torch.nn as nn\n",
    "import torch.optim as optim, torch.nn.functional as F, torchaudio\n",
    "import torchaudio.transforms as transforms, torch.utils.checkpoint as checkpoint\n",
    "import torch.utils.tensorboard as tensorboard, torch.optim.lr_scheduler as lr_scheduler\n",
    "import transformers, neologdn, evaluate, MeCab, deepl, logging, datasets, tqdm, whisper\n",
    "import transformers.utils.logging\n",
    "from datasets import load_from_disk\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Subset\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "from torch.profiler import profile, ProfilerActivity, record_function\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import amp, Tensor\n",
    "from torch.optim import Adamax\n",
    "import logging\n",
    "from transformers import (\n",
    "    logging,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    PretrainedConfig,\n",
    "    GenerationConfig,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizerFast,\n",
    "    WhisperTokenizer,\n",
    "    WhisperModel,\n",
    "    WhisperConfig,\n",
    "    Adafactor,\n",
    "    TrainerCallback,\n",
    "    logging\n",
    ")\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "warnings.warn = lambda *args, **kwargs: None\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "checkpointing_args = {\"reentrant\": False}\n",
    "\n",
    "try:\n",
    "    from torch.nn.functional import scaled_dot_product_attention\n",
    "    SDPA_AVAILABLE = True\n",
    "except (ImportError, RuntimeError, OSError):\n",
    "    scaled_dot_product_attention = None\n",
    "    SDPA_AVAILABLE = False\n",
    "\n",
    "from whisper.decoding import decode as decode_function\n",
    "from whisper.decoding import detect_language as detect_language_function\n",
    "from whisper.transcribe import transcribe as transcribe_function\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "transformers.utils.logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelDimensions:\n",
    "    n_mels: int\n",
    "    n_audio_ctx: int\n",
    "    n_audio_state: int\n",
    "    n_audio_head: int\n",
    "    n_audio_layer: int\n",
    "    n_vocab: int\n",
    "    n_text_ctx: int\n",
    "    n_text_state: int\n",
    "    n_text_head: int\n",
    "    n_text_layer: int\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "class Linear(nn.Linear):\n",
    "    def __init__(self, *args, dropout_rate: float = 0.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.reset_parameters()\n",
    "        self.dropout = nn.Dropout(dropout_rate)  \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        weight = self.weight.to(x.dtype)\n",
    "        bias = None if self.bias is None else self.bias.to(x.dtype)\n",
    "        x = F.linear(x, weight, bias)  \n",
    "        return self.dropout(x) \n",
    "    \n",
    "class Conv1d(nn.Conv1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, nonlinearity='relu')\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def _conv_forward(self, x: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor:\n",
    "        weight = self.weight.to(x.dtype)\n",
    "        bias = None if self.bias is None else self.bias.to(x.dtype)\n",
    "        return super()._conv_forward(x, weight, bias)\n",
    "\n",
    "class RotationLayer(nn.Module):\n",
    "    def __init__(self, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.rotation_matrix = nn.Parameter(torch.eye(embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the rotational transformation\n",
    "        rotated_x = torch.matmul(x, self.rotation_matrix)\n",
    "        return rotated_x\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.orthogonal_(self.rotation_matrix)\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, base=10000):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def rotate_queries_or_keys(self, x):\n",
    "        sinusoid_inp = torch.einsum('i , j -> i j', torch.arange(x.shape[1], device=x.device), self.inv_freq) \n",
    "        sin = sinusoid_inp.sin()[None, :, None, :] \n",
    "        cos = sinusoid_inp.cos()[None, :, None, :]\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        x = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "        return x\n",
    "\n",
    "class SinusoidalFeatures:\n",
    "    def __init__(self, n_ctx, n_state):\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_state = n_state\n",
    "        self.features = self.sinusoidal_features(n_ctx, n_state)\n",
    "\n",
    "    @staticmethod\n",
    "    def sinusoidal_features(n_ctx, n_state):\n",
    "        position = torch.arange(0, n_ctx, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, n_state, 2).float() * -(math.log(10000.0) / n_state))\n",
    "        features = torch.zeros(n_ctx, n_state)\n",
    "        features[:, 0::2] = torch.sin(position * div_term)\n",
    "        features[:, 1::2] = torch.cos(position * div_term)\n",
    "        return features\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.features\n",
    "\n",
    "class LearnedSinusoidalEmbeddings(nn.Module): \n",
    "    def __init__(self, n_ctx, n_state, gradient_checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_state = n_state\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "\n",
    "        sinusoidal_embeddings = SinusoidalFeatures(n_ctx, n_state)()\n",
    "        self.positional_embeddings = nn.Parameter(sinusoidal_embeddings)\n",
    "\n",
    "    def forward(self, positions):\n",
    "        if self.gradient_checkpointing:\n",
    "            position_embeddings = checkpoint.checkpoint(lambda x: self.positional_embeddings[x], positions)\n",
    "        else:\n",
    "            position_embeddings = self.positional_embeddings[positions]\n",
    "\n",
    "        position_embeddings = F.normalize(position_embeddings, p=2, dim=-1)  \n",
    "        return position_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasedCrossAttention(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.n_state = n_state\n",
    "        self.head_dim = n_state // n_head\n",
    "\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(n_head, self.head_dim))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.norm = nn.LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None):\n",
    "        q = self.query(q).view(q.shape[0], q.shape[1], self.n_head, self.head_dim)\n",
    "        k = self.key(k).view(k.shape[0], k.shape[1], self.n_head, self.head_dim)\n",
    "        v = self.value(v).view(v.shape[0], v.shape[1], self.n_head, self.head_dim)\n",
    "\n",
    "        qk = (q @ k.transpose(-2, -1)) / self.head_dim ** 0.5 + self.bias\n",
    "        if mask is not None:\n",
    "            qk += mask\n",
    "\n",
    "        w = F.softmax(qk, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "\n",
    "        out = (w @ v).permute(0, 2, 1, 3).reshape(q.shape[0], q.shape[1], -1)\n",
    "        out = self.norm(self.out(out) + q.view(q.shape[0], q.shape[1], -1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedMemory(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, memory_size: int, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = n_state // n_head\n",
    "        self.memory_size = memory_size\n",
    "\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.memory = nn.Parameter(torch.randn(memory_size, n_state))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        q = self.query(x).view(x.size(0), x.size(1), self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(self.memory).view(self.memory_size, self.n_head, self.head_dim).transpose(1, 0)\n",
    "        v = self.value(self.memory).view(self.memory_size, self.n_head, self.head_dim).transpose(1, 0)\n",
    "\n",
    "        qk = torch.einsum('bhqd,hnd->bhnq', q, k) / self.head_dim ** 0.5  # Adjusting einsum subscripts\n",
    "        w = F.softmax(qk, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "\n",
    "        out = torch.einsum('bhnq,hnd->bhqd', w, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(x.size(0), -1, self.n_head * self.head_dim)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DynamicConvAttention(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, kernel_size=3, dropout_rate=0.1, group_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_state = n_state\n",
    "        self.n_head = n_head\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv1d(n_state, n_state, kernel_size, padding=kernel_size//2, groups=n_head)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.query = Linear(n_state, n_state)\n",
    "        self.key = Linear(n_state, n_state, bias=False)\n",
    "        self.value = Linear(n_state, n_state)\n",
    "        self.out = Linear(n_state, n_state)\n",
    "        \n",
    "        if group_norm:\n",
    "            self.norm = nn.GroupNorm(num_groups=n_head, num_channels=n_state)\n",
    "        else:\n",
    "            self.norm = nn.LayerNorm(n_state)\n",
    "        \n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        if embed_dim != self.n_state:\n",
    "            raise ValueError(f\"Expected embed_dim of {self.n_state}, but got {embed_dim}\")\n",
    "\n",
    "        x = x.permute(0, 2, 1) \n",
    "        conv_out = self.conv(x)\n",
    "        conv_out = conv_out.permute(0, 2, 1) \n",
    "        conv_out = self.norm(conv_out)\n",
    "        conv_out = self.dropout(conv_out)\n",
    "        return self.out(conv_out) + x.permute(0, 2, 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HybridAttention(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, window_size=1, dropout_rate=0.1, group_norm=False):\n",
    "        super().__init__()\n",
    "        self.local_attn = nn.MultiheadAttention(n_state, n_head, dropout=dropout_rate)\n",
    "        self.global_attn = nn.MultiheadAttention(n_state, n_head, dropout=dropout_rate)\n",
    "        self.group_norm = group_norm\n",
    "        self.ln_local = GroupNorm(num_groups=1, num_channels=n_state) if group_norm else LayerNorm(n_state)\n",
    "        self.ln_global = GroupNorm(num_groups=1, num_channels=n_state) if group_norm else LayerNorm(n_state)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x_local = self.ln_local(x)\n",
    "        x_global = self.ln_global(x)\n",
    "        x_local = x_local.permute(1, 0, 2)\n",
    "        x_global = x_global.permute(1, 0, 2)\n",
    "        local_out = self.sliding_window_attention(x_local)\n",
    "        global_out, _ = self.global_attn(x_global, x_global, x_global)\n",
    "        combined_out = local_out + global_out\n",
    "        combined_out = combined_out.permute(1, 0, 2)\n",
    "        return self.dropout(combined_out)\n",
    "\n",
    "    def sliding_window_attention(self, x):\n",
    "        seq_len, batch_size, n_state = x.size()\n",
    "        window_size = min(self.window_size, max(1, seq_len // 4)) \n",
    "        output = torch.zeros_like(x, device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        for i in range(0, seq_len, window_size):\n",
    "            end = min(i + window_size, seq_len)\n",
    "            query = x[i:end, :, :]\n",
    "            start = max(0, i - window_size)\n",
    "            key = x[start:end, :, :]\n",
    "            value = x[start:end, :, :]\n",
    "            attn_output, _ = self.local_attn(query, key, value)\n",
    "            output[i:end, :, :] = attn_output[:end - i, :, :]\n",
    "            \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GroupNorm(nn.Module):  # GroupNorm with RMS\n",
    "    def __init__(self, num_groups, num_channels, eps=1e-6):\n",
    "        super(GroupNorm, self).__init__()\n",
    "        self.num_groups = num_groups\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(num_channels))\n",
    "        self.b = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.scale = nn.Parameter(torch.ones(num_channels))\n",
    "\n",
    "        self.group_norm = nn.GroupNorm(num_groups, num_channels, eps=eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Permute to (batch_size, channels, seq_length)\n",
    "        x = self.group_norm(x)\n",
    "        norm_x = torch.norm(x, dim=-1, keepdim=True)\n",
    "        rms_x = x / norm_x\n",
    "        scaled_x = rms_x * self.scale.view(1, -1, 1)\n",
    "        x = self.g.view(1, -1, 1) * scaled_x + self.b.view(1, -1, 1)\n",
    "        x = x.permute(0, 2, 1)  # Permute back\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    sdpa = True\n",
    "\n",
    "    def __init__(self, n_state: int, n_head: int, dropout_rate=0.01, gradient_checkpointing=False, group_norm=False):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.n_state = n_state\n",
    "        self.head_dim = n_state // n_head\n",
    "\n",
    "        self.query = Linear(n_state, n_state)\n",
    "        self.key = Linear(n_state, n_state, bias=False)\n",
    "        self.value = Linear(n_state, n_state)\n",
    "        self.out = Linear(n_state, n_state)\n",
    "\n",
    "        self.rotary_emb = RotaryEmbedding(dim=self.head_dim)\n",
    "        self.rotation_layer = RotationLayer(self.head_dim) \n",
    "        self.temperature = nn.Parameter(torch.ones(1) * (self.head_dim ** -0.5))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.group_norm = group_norm\n",
    "        self.attn_ln = GroupNorm(num_groups=1, num_channels=n_state) if group_norm else nn.LayerNorm(n_state)\n",
    "\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "\n",
    "    def forward(self, x: Tensor, xa: Optional[Tensor] = None, mask: Optional[Tensor] = None, kv_cache: Optional[dict] = None):\n",
    "        x_norm = self.attn_ln(x)\n",
    "\n",
    "        q = self.query(x_norm)\n",
    "        k = self.key(x_norm if xa is None else xa)\n",
    "        v = self.value(x_norm if xa is None else xa)\n",
    "\n",
    "        if kv_cache is not None and self.key in kv_cache:\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "\n",
    "        q = q.view(q.shape[0], q.shape[1], self.n_head, -1)\n",
    "        k = k.view(k.shape[0], k.shape[1], self.n_head, -1)\n",
    "\n",
    "        # Apply rotary embeddings and rotational layer\n",
    "        q = self.rotary_emb.rotate_queries_or_keys(q)\n",
    "        k = self.rotary_emb.rotate_queries_or_keys(k)\n",
    "        q = self.rotation_layer(q)\n",
    "        k = self.rotation_layer(k)\n",
    "\n",
    "        q = q.view(q.shape[0], q.shape[1], -1)\n",
    "        k = k.view(k.shape[0], k.shape[1], -1)\n",
    "\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "\n",
    "        return self.out(wv) + x, qk\n",
    "\n",
    "    def qkv_attention(self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        n_batch, n_ctx, n_state = q.shape\n",
    "        scale = self.temperature\n",
    "\n",
    "        q = q.view(n_batch, n_ctx, self.n_head, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = k.view(n_batch, k.shape[1], self.n_head, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = v.view(n_batch, v.shape[1], self.n_head, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        if SDPA_AVAILABLE and MultiHeadAttention.sdpa:\n",
    "            a = scaled_dot_product_attention(q, k, v, is_causal=mask is not None and n_ctx > 1)\n",
    "            out = a.permute(0, 2, 1, 3).reshape(n_batch, n_ctx, n_state)\n",
    "            qk = None\n",
    "        else:\n",
    "            qk = (q * scale) @ (k.transpose(-2, -1) * scale)\n",
    "            if mask is not None:\n",
    "                qk += mask[:n_ctx, :n_ctx]\n",
    "            qk = qk.float()\n",
    "\n",
    "            w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "            w = self.dropout(w)\n",
    "            out = (w @ v).permute(0, 2, 1, 3).reshape(n_batch, n_ctx, n_state)\n",
    "            qk = qk.detach()\n",
    "\n",
    "        return out, qk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, dynamic_conv: bool = False, hybrid_attention: bool = False,\n",
    "                 biased_attention: bool = False, augmented_memory: bool = False,\n",
    "                 cross_attention: bool = False, dropout_rate=0.1, gradient_checkpointing=False, group_norm=False, \n",
    "                 window_size: int=1, use_rotation_dynamics=False, use_dynamic_attention_integration=False):\n",
    "        super().__init__()\n",
    "        self.query = Linear(n_state, n_state)\n",
    "        self.key = Linear(n_state, n_state, bias=False)\n",
    "        self.value = Linear(n_state, n_state)\n",
    "        self.out = Linear(n_state, n_state)\n",
    "\n",
    "        self.use_rotation_dynamics = use_rotation_dynamics\n",
    "        self.use_dynamic_attention_integration = use_dynamic_attention_integration\n",
    "\n",
    "        self.hybrid_attention = hybrid_attention\n",
    "        self.dynamic_conv = dynamic_conv\n",
    "        self.biased_attention = biased_attention\n",
    "        self.augmented_memory = augmented_memory\n",
    "\n",
    "        self.attention_mechanisms = nn.ModuleList([MultiHeadAttention(n_state, n_head, dropout_rate=dropout_rate, gradient_checkpointing=gradient_checkpointing, group_norm=group_norm)])\n",
    "        \n",
    "        if self.hybrid_attention:\n",
    "            self.attention_mechanisms.append(HybridAttention(n_state, n_head, window_size=window_size, dropout_rate=dropout_rate, group_norm=group_norm))\n",
    "        if self.dynamic_conv:\n",
    "            self.attention_mechanisms.append(DynamicConvAttention(n_state, n_head, dropout_rate=dropout_rate))\n",
    "        if self.biased_attention:\n",
    "            self.attention_mechanisms.append(BiasedCrossAttention(n_state, n_head, dropout_rate=dropout_rate))\n",
    "        if self.augmented_memory:\n",
    "            self.attention_mechanisms.append(AugmentedMemory(n_state, memory_size=512, n_head=n_head, dropout_rate=dropout_rate))\n",
    "\n",
    "        self.attn_scores = nn.Parameter(torch.randn(len(self.attention_mechanisms)))\n",
    "        self.attn_ln = GroupNorm(num_groups=1, num_channels=n_state) if group_norm else nn.LayerNorm(n_state)\n",
    "        self.cross_attention = cross_attention\n",
    "        if self.cross_attention:\n",
    "            self.cross_attn = MultiHeadAttention(n_state, n_head, dropout_rate=dropout_rate, gradient_checkpointing=gradient_checkpointing, group_norm=group_norm)\n",
    "            self.cross_attn_ln = GroupNorm(num_groups=1, num_channels=n_state) if group_norm else nn.LayerNorm(n_state)\n",
    "\n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp = nn.Sequential(\n",
    "            Linear(n_state, n_mlp),\n",
    "            GroupNorm(num_groups=4, num_channels=n_mlp) if group_norm else nn.LayerNorm(n_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            Linear(n_mlp, n_state)\n",
    "        )\n",
    "        self.mlp_ln = GroupNorm(num_groups=1, num_channels=n_state) if group_norm else nn.LayerNorm(n_state)\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "\n",
    "    def forward(self, x: Tensor, xa: Optional[Tensor] = None, mask: Optional[Tensor] = None, kv_cache: Optional[dict] = None, \n",
    "                biased_attention=False, augmented_memory=False):\n",
    "        global printed_attention_mechanisms_once\n",
    "        if not printed_attention_mechanisms_once:\n",
    "            print(\"Attention mechanisms being used:\")\n",
    "            for attn in self.attention_mechanisms:\n",
    "                print(f\"- {type(attn).__name__}\")\n",
    "            printed_attention_mechanisms_once = True\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(xa if xa is not None else x)\n",
    "        v = self.value(xa if xa is not None else x)\n",
    "\n",
    "        attn_input = self.attn_ln(x)\n",
    "        attn_outputs = []\n",
    "\n",
    "        for attn in self.attention_mechanisms:\n",
    "            if isinstance(attn, BiasedCrossAttention) and not biased_attention:\n",
    "                continue\n",
    "            if isinstance(attn, AugmentedMemory) and not augmented_memory:\n",
    "                continue\n",
    "\n",
    "            if self.gradient_checkpointing:\n",
    "                output = checkpoint.checkpoint(attn, attn_input, k, v, mask) if isinstance(attn, BiasedCrossAttention) else checkpoint.checkpoint(attn, attn_input)\n",
    "            else:\n",
    "                output = attn(attn_input, k, v, mask) if isinstance(attn, BiasedCrossAttention) else attn(attn_input)\n",
    "\n",
    "            attn_outputs.append(output if isinstance(output, Tensor) else output[0])\n",
    "\n",
    "        if not attn_outputs:\n",
    "            raise ValueError(\"No attention mechanisms are enabled. Ensure at least one attention mechanism is active.\")\n",
    "\n",
    "        if self.use_dynamic_attention_integration:\n",
    "            attn_outputs = torch.stack(attn_outputs, dim=0)\n",
    "            attn_scores = F.softmax(self.attn_scores, dim=0)\n",
    "            weighted_attn_output = torch.einsum('i,ijkm->ijkm', attn_scores, attn_outputs).sum(dim=0)\n",
    "            attn_out = x + weighted_attn_output\n",
    "        else:\n",
    "            attn_out = x + attn_outputs[0]\n",
    "\n",
    "        if self.cross_attention and xa is not None:\n",
    "            cross_attn_input = self.cross_attn_ln(attn_out)\n",
    "            if self.gradient_checkpointing:\n",
    "                attn_out = attn_out + checkpoint.checkpoint(self.cross_attn, cross_attn_input, xa, kv_cache)[0]\n",
    "            else:\n",
    "                attn_out = attn_out + self.cross_attn(cross_attn_input, xa, kv_cache=kv_cache)[0]\n",
    "\n",
    "        mlp_input = self.mlp_ln(attn_out)\n",
    "        if self.gradient_checkpointing:\n",
    "            mlp_out = attn_out + checkpoint.checkpoint(self.mlp, mlp_input)\n",
    "        else:\n",
    "            mlp_out = attn_out + self.mlp(mlp_input)\n",
    "\n",
    "        return mlp_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEncoder(nn.Module):\n",
    "    main_input_name = \"input_features\"\n",
    "\n",
    "    def __init__(self, window_size: int, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int,\n",
    "                 dropout_rate=0.1, gradient_checkpointing=False, dynamic_conv: bool = False, hybrid_attention: bool = False, biased_attention: bool = False, augmented_memory: bool = False, group_norm=False,\n",
    "                 use_rotation_dynamics=False, use_dynamic_attention_integration=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.query = Linear(n_state, n_state)\n",
    "        self.key = Linear(n_state, n_state, bias=False)\n",
    "        self.value = Linear(n_state, n_state)\n",
    "        self.out = Linear(n_state, n_state)\n",
    "\n",
    "        self.conv1 = Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "        self.conv2 = Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "        self.group_norm = group_norm\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResidualAttentionBlock(\n",
    "                n_state, n_head,\n",
    "                window_size=window_size,\n",
    "                hybrid_attention=hybrid_attention,                \n",
    "                dynamic_conv=dynamic_conv,\n",
    "                biased_attention=biased_attention,\n",
    "                augmented_memory=augmented_memory,\n",
    "                dropout_rate=dropout_rate,\n",
    "                gradient_checkpointing=gradient_checkpointing,\n",
    "                group_norm=group_norm,\n",
    "                use_rotation_dynamics=use_rotation_dynamics,  \n",
    "                use_dynamic_attention_integration=use_dynamic_attention_integration \n",
    "            )\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "\n",
    "        if self.group_norm:\n",
    "            self.ln_post = GroupNorm(num_groups=1, num_channels=n_state)\n",
    "        else:\n",
    "            self.ln_post = LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = F.gelu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            if self.gradient_checkpointing:\n",
    "                x = checkpoint.checkpoint(block, x)\n",
    "            else:\n",
    "                x = block(x)\n",
    "\n",
    "        x = self.ln_post(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDecoder(nn.Module):\n",
    "    def __init__(self, n_vocab: int, n_ctx: int, n_state: int, n_head: int, n_layer: int,\n",
    "                 dropout_rate=0.1, gradient_checkpointing=False, dynamic_conv: bool = False, hybrid_attention: bool = False, biased_attention: bool = False, augmented_memory: bool = False, group_norm=False,\n",
    "                 use_rotation_dynamics=False, use_dynamic_attention_integration=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.query = Linear(n_state, n_state)\n",
    "        self.key = Linear(n_state, n_state, bias=False)\n",
    "        self.value = Linear(n_state, n_state)\n",
    "        self.out = Linear(n_state, n_state)\n",
    "\n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_state)\n",
    "        self.positional_embedding = LearnedSinusoidalEmbeddings(n_ctx, n_state, gradient_checkpointing=gradient_checkpointing)\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "        self.group_norm = group_norm\n",
    "\n",
    "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList([\n",
    "            ResidualAttentionBlock(\n",
    "                n_state, n_head,\n",
    "                hybrid_attention=hybrid_attention,\n",
    "                dynamic_conv=dynamic_conv,\n",
    "                biased_attention=biased_attention,\n",
    "                augmented_memory=augmented_memory,\n",
    "                cross_attention=True,\n",
    "                dropout_rate=dropout_rate,\n",
    "                gradient_checkpointing=gradient_checkpointing,\n",
    "                group_norm=group_norm,\n",
    "                use_rotation_dynamics=use_rotation_dynamics,  # New toggle\n",
    "                use_dynamic_attention_integration=use_dynamic_attention_integration  # New toggle\n",
    "            )\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "\n",
    "        if self.group_norm:\n",
    "            self.ln_post = GroupNorm(num_groups=1, num_channels=n_state)\n",
    "        else:\n",
    "            self.ln_post = LayerNorm(n_state)\n",
    "\n",
    "        mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)\n",
    "        self.register_buffer('mask', mask, persistent=False)\n",
    "\n",
    "    def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):\n",
    "        \n",
    "        positions = torch.arange(x.shape[1], device=x.device)\n",
    "        pos_emb = self.positional_embedding(positions).unsqueeze(0)\n",
    "        x = self.token_embedding(x) + pos_emb\n",
    "        x = x.to(xa.dtype)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            if self.gradient_checkpointing:\n",
    "                x = checkpoint.checkpoint(block, x, xa, self.mask, kv_cache)\n",
    "            else:\n",
    "                x = block(x, xa, self.mask, kv_cache)\n",
    "\n",
    "        x = self.ln_post(x)\n",
    "        logits = (x @ self.token_embedding.weight.to(x.dtype).T).float()\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperConfig(PretrainedConfig):\n",
    "    model_type = \"whisper\"\n",
    "    keys_to_ignore_at_inference = []\n",
    "    attribute_map = {}\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation_dropout = kwargs.get(\"activation_dropout\", 0.0)\n",
    "        self.activation_function = kwargs.get(\"activation_function\", \"gelu\")\n",
    "        self.architectures = kwargs.get(\"architectures\", [\"WhisperForConditionalGeneration\"])\n",
    "        self.attention_dropout = kwargs.get(\"attention_dropout\", 0.0)\n",
    "        self.begin_suppress_tokens = kwargs.get(\"begin_suppress_tokens\", [220, 50257])\n",
    "        self.bos_token_id = kwargs.get(\"bos_token_id\", 50257)\n",
    "        self.d_model = kwargs.get(\"d_model\", 1024)\n",
    "        self.decoder_attention_heads = kwargs.get(\"decoder_attention_heads\", 16)\n",
    "        self.decoder_ffn_dim = kwargs.get(\"decoder_ffn_dim\", 4096)\n",
    "        self.decoder_layerdrop = kwargs.get(\"decoder_layerdrop\", 0.0)\n",
    "        self.decoder_layers = kwargs.get(\"decoder_layers\", 24)\n",
    "        self.decoder_start_token_id = kwargs.get(\"decoder_start_token_id\", 50258)\n",
    "        self.dropout = kwargs.get(\"dropout\", 0.0)\n",
    "        self.encoder_attention_heads = kwargs.get(\"encoder_attention_heads\", 16)\n",
    "        self.encoder_ffn_dim = kwargs.get(\"encoder_ffn_dim\", 4096)\n",
    "        self.encoder_layerdrop = kwargs.get(\"encoder_layerdrop\", 0.0)\n",
    "        self.encoder_layers = kwargs.get(\"encoder_layers\", 24)\n",
    "        self.eos_token_id = kwargs.get(\"eos_token_id\", 50257)\n",
    "        self.forced_decoder_ids = kwargs.get(\"forced_decoder_ids\", [[1, 50259], [2, 50359], [3, 50363]])\n",
    "        self.init_std = kwargs.get(\"init_std\", 0.02)\n",
    "        self.is_encoder_decoder = kwargs.get(\"is_encoder_decoder\", True)\n",
    "        self.max_length = kwargs.get(\"max_length\", 448)\n",
    "        self.max_source_positions = kwargs.get(\"max_source_positions\", 1500)\n",
    "        self.max_target_positions = kwargs.get(\"max_target_positions\", 448)\n",
    "        self.num_hidden_layers = kwargs.get(\"num_hidden_layers\", 24)\n",
    "        self.num_mel_bins = kwargs.get(\"num_mel_bins\", 80)\n",
    "        self.pad_token_id = kwargs.get(\"pad_token_id\", 50257)\n",
    "        self.scale_embedding = kwargs.get(\"scale_embedding\", False)\n",
    "        self.suppress_tokens = kwargs.get(\"suppress_tokens\", [\n",
    "            1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873,\n",
    "            893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585,\n",
    "            6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553,\n",
    "            16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865,\n",
    "            42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362\n",
    "        ])\n",
    "        self.torch_dtype = kwargs.get(\"torch_dtype\", \"float32\")\n",
    "        self.transformers_version = kwargs.get(\"transformers_version\", \"4.27.0.dev0\")\n",
    "        self.cache = kwargs.get(\"cache\", True)\n",
    "        self.vocab_size = kwargs.get(\"vocab_size\", 51865)\n",
    "\n",
    "        # Compatibility mappings\n",
    "        self.n_vocab = self.vocab_size\n",
    "        self.n_mels = self.num_mel_bins\n",
    "        self.n_audio_ctx = self.max_source_positions\n",
    "        self.n_audio_state = self.d_model\n",
    "        self.n_audio_head = self.encoder_attention_heads\n",
    "        self.n_audio_layer = self.encoder_layers\n",
    "        self.n_text_ctx = self.max_target_positions\n",
    "        self.n_text_state = self.d_model\n",
    "        self.n_text_head = self.decoder_attention_heads\n",
    "        self.n_text_layer = self.decoder_layers\n",
    "        self.dropout_rate = self.dropout\n",
    "\n",
    "        super().__init__(\n",
    "            pad_token_id=kwargs.get(\"pad_token_id\", 50256),\n",
    "            bos_token_id=kwargs.get(\"bos_token_id\", 50256),\n",
    "            eos_token_id=kwargs.get(\"eos_token_id\", 50256),\n",
    "            is_encoder_decoder=kwargs.get(\"is_encoder_decoder\", True),\n",
    "            decoder_start_token_id=kwargs.get(\"decoder_start_token_id\", 50256),\n",
    "            suppress_tokens=kwargs.get(\"suppress_tokens\", None),\n",
    "            begin_suppress_tokens=kwargs.get(\"begin_suppress_tokens\", [220, 50256]),\n",
    "            **kwargs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Whisper(nn.Module):\n",
    "    def __init__(self, config: WhisperConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.generation_config = GenerationConfig()\n",
    "        self.encoder = AudioEncoder(\n",
    "            self.config.window_size,\n",
    "            self.config.n_mels,\n",
    "            self.config.n_audio_ctx,\n",
    "            self.config.n_audio_state,\n",
    "            self.config.n_audio_head,\n",
    "            self.config.n_audio_layer,\n",
    "            self.config.dropout_rate,\n",
    "            self.config.gradient_checkpointing,\n",
    "            self.config.dynamic_conv,\n",
    "            self.config.hybrid_attention,\n",
    "            self.config.biased_attention,\n",
    "            self.config.augmented_memory,\n",
    "            self.config.group_norm,\n",
    "            self.config.use_rotation_dynamics,\n",
    "            self.config.use_dynamic_attention_integration \n",
    "        )\n",
    "        self.decoder = TextDecoder(\n",
    "            self.config.n_vocab,\n",
    "            self.config.n_text_ctx,\n",
    "            self.config.n_text_state,\n",
    "            self.config.n_text_head,\n",
    "            self.config.n_text_layer,\n",
    "            self.config.dropout_rate,\n",
    "            self.config.dynamic_conv,\n",
    "            self.config.hybrid_attention,\n",
    "            self.config.biased_attention,\n",
    "            self.config.augmented_memory,\n",
    "            self.config.group_norm,\n",
    "            self.config.use_rotation_dynamics, \n",
    "            self.config.use_dynamic_attention_integration\n",
    "        )\n",
    "\n",
    "        all_heads = torch.zeros(\n",
    "            self.config.n_text_layer, self.config.n_text_head, dtype=torch.bool\n",
    "        )\n",
    "        all_heads[self.config.n_text_layer // 2:] = True\n",
    "        self.register_buffer('alignment_heads', all_heads.to_sparse(), persistent=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int) -> torch.Tensor:\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "        shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\"pad_token_id has to be defined.\")\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "        return shifted_input_ids\n",
    "\n",
    "    def forward(self, input_features: torch.Tensor, labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
    "        encoded_features = self.encoder(input_features)\n",
    "        \n",
    "        if labels is not None:\n",
    "            decoder_input_ids = self.shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
    "        else:\n",
    "            decoder_input_ids = None\n",
    "        \n",
    "        logits = self.decoder(decoder_input_ids, encoded_features)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            logits = logits.view(-1, self.config.n_vocab)  \n",
    "            labels = labels.view(-1).long()  \n",
    "            loss = loss_fct(logits, labels)  \n",
    "        return {\"loss\": loss, \"logits\": logits, \"input_features\": encoded_features}\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @property\n",
    "    def is_multilingual(self):\n",
    "        return self.config.n_vocab >= len(tokenizer)\n",
    "\n",
    "    @property\n",
    "    def num_languages(self):\n",
    "        return self.config.n_vocab - (len(tokenizer)-100)\n",
    "\n",
    "    def set_alignment_heads(self, dump: bytes):\n",
    "        array = np.frombuffer(\n",
    "            gzip.decompress(base64.b85decode(dump)), dtype=bool\n",
    "        ).copy()\n",
    "        mask = torch.from_numpy(array).reshape(\n",
    "            self.config.n_text_layer, self.config.n_text_head\n",
    "        )\n",
    "        self.register_buffer('alignment_heads', mask.to_sparse(), persistent=False)\n",
    "\n",
    "    def embed_aud(self, input_features: torch.Tensor):\n",
    "        return self.encoder(input_features)\n",
    "\n",
    "    def logits(self, input_ids: torch.Tensor, input_features: torch.Tensor):\n",
    "        return self.decoder(input_ids, input_features)\n",
    "\n",
    "    def install_kv_cache_hooks(self, cache: Optional[dict] = None):\n",
    "        cache = {**cache} if cache is not None else {}\n",
    "        hooks = []\n",
    "\n",
    "        def save_to_cache(module, _, output):\n",
    "            if module not in cache or output.shape[1] > self.config.n_text_ctx:\n",
    "                cache[module] = output\n",
    "            else:\n",
    "                cache[module] = torch.cat([cache[module], output], dim=1).detach()\n",
    "            return cache[module]\n",
    "\n",
    "        def install_hooks(layer: nn.Module):\n",
    "            if isinstance(layer, MultiHeadAttention):\n",
    "                hooks.append(layer.key.register_forward_hook(save_to_cache))\n",
    "                hooks.append(layer.value.register_forward_hook(save_to_cache))\n",
    "\n",
    "        self.decoder.apply(install_hooks)\n",
    "        return cache, hooks\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings: torch.nn.Embedding):\n",
    "        self.decoder.token_embedding = new_embeddings\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.decoder.token_embedding\n",
    "\n",
    "    def resize_token_embeddings(self, new_num_tokens: int):\n",
    "        old_embeddings = self.get_input_embeddings()\n",
    "        old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n",
    "        new_embeddings = torch.nn.Embedding(new_num_tokens, old_embedding_dim)\n",
    "    \n",
    "        new_embeddings.weight.data[:old_num_tokens, :] = old_embeddings.weight.data\n",
    "        self.set_input_embeddings(new_embeddings)\n",
    "        self.config.n_vocab = new_num_tokens\n",
    "\n",
    "    detect_language = detect_language_function\n",
    "    transcribe = transcribe_function\n",
    "    decode = decode_function\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, ignore_mismatched_sizes=False, **kwargs):\n",
    "        config = WhisperConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "        model = cls(config, **kwargs)\n",
    "        state_dict = torch.load(os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin\"), map_location=\"cpu\")\n",
    "        \n",
    "        if ignore_mismatched_sizes:\n",
    "            model_state_dict = model.state_dict()\n",
    "            for key in state_dict.keys():\n",
    "                if key in model_state_dict and state_dict[key].size() != model_state_dict[key].size():\n",
    "                    print(f\"Skipping loading of {key} due to size mismatch\")\n",
    "                    state_dict[key] = model_state_dict[key]\n",
    "        \n",
    "        model.load_state_dict(state_dict, strict=not ignore_mismatched_sizes)\n",
    "        return model\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        return {'input_features': input_ids}\n",
    "\n",
    "    def _prepare_decoder_input_ids_for_generation(self, batch_size, decoder_start_token_id=None, bos_token_id=None):\n",
    "        return torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * self.config.decoder_start_token_id\n",
    "\n",
    "    def can_generate(self):\n",
    "        return True\n",
    "    \n",
    "    def generate(self, inputs, **kwargs):\n",
    "        encoder_outputs = self.encoder(inputs)\n",
    "        decoder_input_ids = torch.zeros((inputs.size(0), 1), dtype=torch.long, device=inputs.device)\n",
    "        outputs = self.decoder(decoder_input_ids, encoder_outputs)\n",
    "        return outputs.argmax(dim=-1)\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        self.config.save_pretrained(save_directory)\n",
    "        torch.save(self.state_dict(), os.path.join(save_directory, \"pytorch_model.bin\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wave(wave_path, sample_rate: int = 16000) -> torch.Tensor:\n",
    "    waveform, sr = torchaudio.load(wave_path, normalize=True)\n",
    "    if sample_rate != sr:\n",
    "        waveform = torchaudio.transforms.Resample(sr, sample_rate)(waveform)\n",
    "    return waveform\n",
    "\n",
    "class audioDataset(Dataset):\n",
    "    def __init__(self, csv_file, aud_dir, tokenizer, sample_rate=16000):\n",
    "        self.aud_dir = aud_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_rate = sample_rate\n",
    "        self.samples = []\n",
    "\n",
    "        with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader) \n",
    "            for row in reader:\n",
    "                aud_path, label = row[0], row[1]\n",
    "                self.samples.append((aud_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        aud_path, label = self.samples[idx]\n",
    "        label = handle_unknown_characters(label)\n",
    "\n",
    "        # label = self.tokenizer.encode(label)\n",
    "        # label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        aud = f'{self.aud_dir}/{aud_path}'\n",
    "        return {\n",
    "            'input_features': aud,\n",
    "            'labels': label,\n",
    "            'input_ids': label \n",
    "        }\n",
    "\n",
    "def handle_unknown_characters(label): \n",
    "    label = label.encode('utf-8').decode('utf-8', errors='replace')\n",
    "    label = neologdn.normalize(label, repeat=1)#, verbose=0, remove_parentheses=True, remove_brackets=True) \n",
    "    return label\n",
    "\n",
    "class WhisperDataCollatorWithPadding:\n",
    "    def __init__(self, tokenizer, n_mels, n_fft, hop_length, sample_rate=16000):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.mel_spectrogram_transform = transforms.MelSpectrogram(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_fft=self.n_fft,\n",
    "            n_mels=self.n_mels,\n",
    "            hop_length=self.hop_length\n",
    "        )\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_features, dec_input_ids, labels = [], [], []\n",
    "\n",
    "        for f in features:\n",
    "            aud_path = f['input_features']\n",
    "            aud, _ = torchaudio.load(aud_path, normalize=True)\n",
    "            aud = whisper.pad_or_trim(aud.flatten())\n",
    "\n",
    "            # Apply the custom mel spectrogram transformation\n",
    "            mel_spectrogram = self.mel_spectrogram_transform(aud)\n",
    "            log_mel_spectrogram = torch.log(mel_spectrogram + 1e-8)  # Adding a small value to avoid log(0)\n",
    "\n",
    "            label = handle_unknown_characters(f['labels'])  # Ensure label is properly handled\n",
    "            encoded_input = self.tokenizer.encode(label)\n",
    "            encoded_label = self.tokenizer.encode(label)\n",
    "\n",
    "            dec_input_ids.append([self.tokenizer.bos_token_id] + encoded_input)\n",
    "            labels.append(encoded_label + [self.tokenizer.eos_token_id])\n",
    "            input_features.append(log_mel_spectrogram)\n",
    "\n",
    "        input_features = torch.stack(input_features)\n",
    "\n",
    "        input_lengths = [len(ids) for ids in dec_input_ids]\n",
    "        label_lengths = [len(lab) for lab in labels]\n",
    "        max_len = max(input_lengths + label_lengths)\n",
    "\n",
    "        dec_input_ids = [np.pad(ids, (0, max_len - len(ids)), 'constant', constant_values=self.tokenizer.pad_token_id) for ids in dec_input_ids]\n",
    "        labels = [np.pad(lab, (0, max_len - len(lab)), 'constant', constant_values=-100) for lab in labels]\n",
    "\n",
    "        batch = {\n",
    "            \"input_ids\": dec_input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"input_features\": input_features\n",
    "        }\n",
    "        batch = {k: torch.tensor(v, requires_grad=False) for k, v in batch.items()}\n",
    "        return batch\n",
    "\n",
    "\n",
    "metrics_cer = evaluate.load(\"cer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred[\"predictions\"]\n",
    "    label_ids = pred[\"label_ids\"]\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = 100 * metrics_cer.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"cer\": cer}\n",
    "\n",
    "checkpoint_dir = 'D:/proj3/checkpoints/'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "log_dir = \"D:/proj3/logs/run_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "writer = SummaryWriter(log_dir)\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(log_dir, 'training.log'), \n",
    "    filemode='w', \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate(model, train_loader, eval_loader, optimizer, scheduler, loss_fn, num_epochs=1, max_steps=None, device='cuda', accumulation_steps=1, clear_cache=True, log_interval=10, eval_interval=20, save_interval=100, checkpoint_dir=\"checkpoint_dir\", log_dir=\"log_dir\"):\n",
    "    model.to(device)\n",
    "    global_step = 0\n",
    "    scaler = GradScaler()\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    lr_warning_printed = False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if max_steps is not None and global_step >= max_steps:\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            if max_steps is not None and global_step >= max_steps:\n",
    "                break\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            input_features = batch['input_features'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].long().to(device)\n",
    "\n",
    "            with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "                with record_function(\"model_training\"):\n",
    "                    with torch.amp.autocast(device_type='cuda'):\n",
    "                        input_features_encoded = model.encoder(input_features)\n",
    "                        decoder_output = model.decoder(input_ids, input_features_encoded)\n",
    "                        logits = decoder_output.view(-1, decoder_output.size(-1))\n",
    "                        loss = loss_fn(logits, labels.view(-1))\n",
    "                        total_loss += loss.item()\n",
    "                        loss = loss / accumulation_steps\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "\n",
    "                    if (step + 1) % accumulation_steps == 0:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        if clear_cache:\n",
    "                            torch.cuda.empty_cache()\n",
    "\n",
    "            global_step += 1\n",
    "            end_time = time.time()\n",
    "            samples_per_sec = len(batch['input_features']) / (end_time - start_time)\n",
    "\n",
    "            total_norm = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** (1. / 2)\n",
    "\n",
    "            if global_step % log_interval == 0:\n",
    "                writer.add_scalar('Loss/train', total_loss / (step + 1), global_step)\n",
    "                writer.add_scalar('GradientNorm', total_norm, global_step)\n",
    "                \n",
    "                lr = optimizer.param_groups[0].get('lr', None)\n",
    "                if lr is not None:\n",
    "                    writer.add_scalar('LearningRate', lr, global_step)\n",
    "                else:\n",
    "                    if not lr_warning_printed:\n",
    "                        print(f\"Warning: Learning rate is None at step {global_step}\")\n",
    "                        lr_warning_printed = True\n",
    "\n",
    "                writer.add_scalar('SamplesPerSec', samples_per_sec, global_step)\n",
    "\n",
    "            if global_step % eval_interval == 0:\n",
    "                model.eval()\n",
    "                eval_loss = 0\n",
    "                all_predictions = []\n",
    "                all_labels = []\n",
    "                with torch.no_grad():\n",
    "                    for eval_batch in eval_loader:\n",
    "                        input_features = eval_batch['input_features'].to(device)\n",
    "                        input_ids = eval_batch['input_ids'].to(device)\n",
    "                        labels = eval_batch['labels'].long().to(device)\n",
    "                        input_features_encoded = model.encoder(input_features)\n",
    "                        decoder_output = model.decoder(input_ids, input_features_encoded)\n",
    "                        logits = decoder_output.view(-1, decoder_output.size(-1))\n",
    "                        loss = loss_fn(logits, labels.view(-1))\n",
    "                        eval_loss += loss.item()\n",
    "                        all_predictions.extend(torch.argmax(decoder_output, dim=-1).cpu().numpy().tolist())\n",
    "                        all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "                eval_loss /= len(eval_loader)\n",
    "                predictions = {\"predictions\": np.array(all_predictions, dtype=\"object\"), \"label_ids\": np.array(all_labels, dtype=\"object\")}\n",
    "                metrics = compute_metrics(predictions)\n",
    "                writer.add_scalar('Loss/eval', eval_loss, global_step)\n",
    "                writer.add_scalar('CER', metrics['cer'], global_step)\n",
    "                scheduler.step()  # Step the scheduler\n",
    "\n",
    "                sample_indices = range(min(1, len(all_predictions))) \n",
    "                for idx in sample_indices:\n",
    "                    pred_str = tokenizer.decode(all_predictions[idx], skip_special_tokens=True)\n",
    "                    label_str = tokenizer.decode(all_labels[idx], skip_special_tokens=True)\n",
    "                    print(f\"Evaluation Loss: {eval_loss:.4f}\")\n",
    "                    print(f\"Evaluation Sample {idx}: Prediction: {pred_str}, Label: {label_str}\")\n",
    "                    logging.info(f\"Evaluation Sample {idx}: Prediction: {pred_str}, Label: {label_str}\")\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                # print(f\"Evaluation Loss: {eval_loss:.4f}\")\n",
    "                # print(f\"Character Error Rate (CER): {metrics['cer']:.4f}\")\n",
    "\n",
    "            if global_step % save_interval == 0:\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_step_{global_step}.pt')\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"Model saved at step {global_step} to {checkpoint_path}\")\n",
    "                logging.info(f\"Model saved at step {global_step} to {checkpoint_path}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}')\n",
    "        logging.info(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "    final_model_path = os.path.join(checkpoint_dir, 'final_model.pt')\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "    logging.info(f\"Final model saved to {final_model_path}\")\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pytorch loop\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     #tokenizer = WhisperTokenizerFast.from_pretrained(\"D:/good/my_tokenizer_with_unk\", task=\"transcribe\", language=\"japanese\", local_files_only=True)\n",
    "#     tokenizer = WhisperTokenizerFast.from_pretrained(\"openai/whisper-large\")\n",
    "#     csv_file = 'D:/proj/datasets/gf_1/metadata.csv'\n",
    "#     audio_dir = 'D:/proj/datasets/gf_1/'\n",
    "\n",
    "#     def train_val_dataset(dataset, val_split=0.001):\n",
    "#         train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "#         datasets = {}\n",
    "#         datasets['train'] = Subset(dataset, train_idx)\n",
    "#         datasets['val'] = Subset(dataset, val_idx)\n",
    "#         return datasets\n",
    "\n",
    "#     dataset = audioDataset(csv_file, audio_dir, tokenizer)\n",
    "#     datasets = train_val_dataset(dataset)\n",
    "#     train_dataset = datasets['train']\n",
    "#     eval_dataset = datasets['val']\n",
    "    \n",
    "#     def train_dataloader():   \n",
    "#         return DataLoader(\n",
    "#             train_dataset,\n",
    "#             batch_size=1,\n",
    "#             drop_last=False, \n",
    "#             shuffle=True, \n",
    "#             num_workers=0,\n",
    "#             collate_fn=collate_fn\n",
    "#         )\n",
    "\n",
    "#     def eval_dataloader():\n",
    "#         return DataLoader(\n",
    "#             eval_dataset,\n",
    "#             batch_size=1, \n",
    "#             drop_last=True,\n",
    "#             shuffle=False,\n",
    "#             num_workers=0,\n",
    "#             collate_fn=collate_fn\n",
    "#         )\n",
    "\n",
    "#     collate_fn = WhisperDataCollatorWithPadding(tokenizer, n_fft=1024, hop_length=256, n_mels=80)\n",
    "#     train_loader = train_dataloader()\n",
    "#     eval_loader = eval_dataloader()\n",
    "\n",
    "#     config = WhisperConfig(\n",
    "#         n_mels=80,\n",
    "#         n_audio_ctx=1500,\n",
    "#         n_audio_state=768, \n",
    "#         n_audio_head=12, \n",
    "#         n_audio_layer=6, \n",
    "#         n_vocab=len(tokenizer),#55116,\n",
    "#         n_text_ctx=448,\n",
    "#         n_text_state=768, \n",
    "#         n_text_head=12, \n",
    "#         n_text_layer=6,\n",
    "#         dropout_rate=0.01,\n",
    "#         max_source_positions=1500,\n",
    "#         window_size=1,\n",
    "#         gradient_checkpointing=False,\n",
    "#         hybrid_attention=False,\n",
    "#         dynamic_conv=False,\n",
    "#         biased_attention=False,\n",
    "#         augmented_memory=False,\n",
    "#         group_norm=False,\n",
    "#         use_rotation_dynamics=False,\n",
    "#         use_dynamic_attention_integration=False,\n",
    "#     )\n",
    "\n",
    "#     model = Whisper(config).cuda()\n",
    "#     # model.resize_token_embeddings(len(tokenizer))\n",
    "#     optimizer = transformers.Adafactor(model.parameters(), \n",
    "#                                     clip_threshold=0.99, \n",
    "#                                     weight_decay=0.025, \n",
    "#                                     scale_parameter=True, \n",
    "#                                     relative_step=False, \n",
    "#                                     warmup_init=False, \n",
    "#                                     lr=2.25e-3)\n",
    "\n",
    "#     scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n",
    "\n",
    "#     loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "#     from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#     train_and_evaluate(model, train_loader, eval_loader, optimizer, scheduler, loss_fn, max_steps=100, num_epochs=1, device='cuda', accumulation_steps=1, clear_cache=True, log_interval=1, eval_interval=10, save_interval=100, checkpoint_dir=checkpoint_dir, log_dir=log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Hugging face\n",
    "\n",
    "old_model = \"openai/whisper-small\"\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(old_model, sampling_rate=16000, local_files_only=True)\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(old_model)\n",
    "proccesor = WhisperProcessor.from_pretrained(old_model, tokenizer=tokenizer, feature_extractor=feature_extractor, local_files_only=True)\n",
    "# Define a global flag\n",
    "global printed_attention_mechanisms_once\n",
    "printed_attention_mechanisms_once = False\n",
    "\n",
    "config = WhisperConfig(\n",
    "    n_mels=80,\n",
    "    n_audio_ctx=1500,\n",
    "    n_audio_state=768, \n",
    "    n_audio_head=12, \n",
    "    n_audio_layer=6, \n",
    "    n_vocab=len(tokenizer),#55116,\n",
    "    n_text_ctx=448,\n",
    "    n_text_state=768, \n",
    "    n_text_head=12, \n",
    "    n_text_layer=6,\n",
    "    dropout_rate=0.01,\n",
    "    max_source_positions=1500,\n",
    "    window_size=40,\n",
    "    gradient_checkpointing=False,\n",
    "    hybrid_attention=False,\n",
    "    dynamic_conv=False,\n",
    "    biased_attention=False,\n",
    "    augmented_memory=False,\n",
    "    group_norm=False,\n",
    "    use_rotation_dynamics=True,\n",
    "    use_dynamic_attention_integration=False,\n",
    ")\n",
    "\n",
    "model = Whisper(config).cuda()\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "optimizer = transformers.Adafactor(model.parameters(), \n",
    "                                clip_threshold=0.99, \n",
    "                                weight_decay=0.025, \n",
    "                                scale_parameter=True, \n",
    "                                relative_step=False, \n",
    "                                warmup_init=False, \n",
    "                                lr=2.25e-3)\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "def get_adamax_optimizer(model, learning_rate=0.001, weight_decay=0.0):\n",
    "    return Adamax(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "ds_a = load_from_disk(\"D:/proj/datasets/gvjas\")[\"train\"].to_iterable_dataset(num_shards=200).filter(lambda x: len(x['sentence']) > 0).map(lambda x: {\"sentence\": neologdn.normalize(x['sentence'], repeat=1)}) \n",
    "ds_b = load_from_disk(\"D:/proj/datasets/gvjas\")[\"test\"].to_iterable_dataset(num_shards=2).filter(lambda x: len(x['sentence']) > 0).map(lambda x: {\"sentence\": neologdn.normalize(x['sentence'], repeat=1)})\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "train = ds_a.map(prepare_dataset).select_columns([\"input_features\", \"labels\"])\n",
    "test = ds_b.map(prepare_dataset).select_columns([\"input_features\", \"labels\"])\n",
    "\n",
    "metric = evaluate.load(\"cer\")\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    proccesor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.proccesor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.proccesor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(proccesor=proccesor, decoder_start_token_id=model.config.decoder_start_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "class CustomTensorBoardCallback(TrainerCallback):\n",
    "    def __init__(self, tb_writer):\n",
    "        self.tb_writer = tb_writer\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            for key, value in logs.items():\n",
    "                self.tb_writer.add_scalar(key, value, state.global_step)\n",
    "            if 'predictions' in logs and 'label_ids' in logs:\n",
    "                cer = compute_cer(logs['predictions'], logs['label_ids'])\n",
    "                self.tb_writer.add_scalar(\"cer\", cer, state.global_step)\n",
    "\n",
    "def compute_cer(predictions, label_ids):\n",
    "    pred_str = proccesor.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    label_str = proccesor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    return 100 * cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    wakati = MeCab.Tagger(\"-Owakati\")   \n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = proccesor.tokenizer.pad_token_id\n",
    "    \n",
    "    pred_str = proccesor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = proccesor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    cer = 100 * cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    pred_flat = np.argmax(pred_ids, axis=2).flatten()\n",
    "    labels_flat = label_ids.flatten()\n",
    "    mask = labels_flat != proccesor.tokenizer.pad_token_id\n",
    "\n",
    "    accuracy = accuracy_score(labels_flat[mask], pred_flat[mask])\n",
    "    precision = precision_score(labels_flat[mask], pred_flat[mask], average='weighted')\n",
    "    recall = recall_score(labels_flat[mask], pred_flat[mask], average='weighted')\n",
    "    f1 = f1_score(labels_flat[mask], pred_flat[mask], average='weighted')\n",
    "    \n",
    "    return {\n",
    "        \"cer\": cer,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "tb_writer = SummaryWriter(log_dir='./output/logs')\n",
    "tb_callback = CustomTensorBoardCallback(tb_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = \"./output\"\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=out,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=1, \n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2.4e-5,\n",
    "    warmup_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=100,\n",
    "    tf32=True,\n",
    "    bf16=True,\n",
    "    save_steps=100,\n",
    "    logging_steps=5,\n",
    "    logging_dir=out+\"/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    report_to=[\"tensorboard\"],\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    hub_private_repo=True,\n",
    "    metric_for_best_model=\"cer\",\n",
    "    predict_with_generate=False,\n",
    "    greater_is_better=False,\n",
    "    generation_max_length=128,\n",
    "    optim=\"adafactor\",\n",
    "    weight_decay=0.0025,\n",
    "    disable_tqdm=False,\n",
    "    save_total_limit=1,  \n",
    "    torch_empty_cache_steps=10,\n",
    "    gradient_checkpointing_kwargs={\"reentrant\": False},\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=proccesor.feature_extractor,\n",
    "    optimizers=(optimizer, scheduler), # optimizers=(get_adamax_optimizer, None)\n",
    "    callbacks=[tb_callback]\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "tb_writer.close()\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
