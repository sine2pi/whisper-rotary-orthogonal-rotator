{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, random, math, time\n",
    "import math\n",
    "import logging, warnings, csv, base64, gzip\n",
    "import json, datetime, numpy as np, torch, torch.nn as nn\n",
    "import torch.optim as optim, torch.nn.functional as F, torchaudio\n",
    "import torchaudio.transforms as transforms, torch.utils.checkpoint as checkpoint\n",
    "import torch.utils.tensorboard as tensorboard, torch.optim.lr_scheduler as lr_scheduler\n",
    "import transformers, neologdn, evaluate, MeCab, deepl, logging, datasets, tqdm, whisper\n",
    "import transformers.utils.logging\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Subset, IterableDataset\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "from torch.profiler import profile, ProfilerActivity, record_function\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import amp, Tensor\n",
    "from torch.optim import Adamax\n",
    "import logging\n",
    "from safetensors import safe_open\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from transformers.trainer_pt_utils import IterableDatasetShard\n",
    "from transformers.trainer_utils import is_main_process\n",
    "from transformers.trainer_pt_utils import find_batch_size, get_parameter_names\n",
    "from transformers import (\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    "    logging,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    PretrainedConfig,\n",
    "    GenerationConfig,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizerFast,\n",
    "    WhisperTokenizer,\n",
    "    WhisperModel,\n",
    "    WhisperConfig,\n",
    "    Adafactor,\n",
    "    TrainerCallback,\n",
    "    logging\n",
    ")\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "warnings.warn = lambda *args, **kwargs: None\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "checkpointing_args = {\"reentrant\": False}\n",
    "\n",
    "try:\n",
    "    from torch.nn.functional import scaled_dot_product_attention\n",
    "    SDPA_AVAILABLE = True\n",
    "except (ImportError, RuntimeError, OSError):\n",
    "    scaled_dot_product_attention = None\n",
    "    SDPA_AVAILABLE = False\n",
    "\n",
    "from whisper.decoding import decode as decode_function\n",
    "from whisper.decoding import detect_language as detect_language_function\n",
    "from whisper.transcribe import transcribe as transcribe_function\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "transformers.utils.logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelDimensions:\n",
    "    n_mels: int\n",
    "    n_audio_ctx: int\n",
    "    n_audio_state: int\n",
    "    n_audio_head: int\n",
    "    n_audio_layer: int\n",
    "    n_vocab: int\n",
    "    n_text_ctx: int\n",
    "    n_text_state: int\n",
    "    n_text_head: int\n",
    "    n_text_layer: int\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "class Linear(nn.Linear):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return F.linear(\n",
    "            x,\n",
    "            self.weight.to(x.dtype),\n",
    "            None if self.bias is None else self.bias.to(x.dtype),\n",
    "        )\n",
    "\n",
    "class Conv1d(nn.Conv1d):\n",
    "    def _conv_forward(\n",
    "        self, x: Tensor, weight: Tensor, bias: Optional[Tensor]\n",
    "    ) -> Tensor:\n",
    "        return super()._conv_forward(\n",
    "            x, weight.to(x.dtype), None if bias is None else bias.to(x.dtype)\n",
    "        )\n",
    "\n",
    "def sinusoids(length, channels, max_timescale=10000):\n",
    "    \"\"\"Returns sinusoids for positional embedding\"\"\"\n",
    "    assert channels % 2 == 0\n",
    "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
    "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n",
    "\n",
    "@contextmanager\n",
    "def disable_sdpa():\n",
    "    prev_state = MultiHeadAttention.use_sdpa\n",
    "    try:\n",
    "        MultiHeadAttention.use_sdpa = False\n",
    "        yield\n",
    "    finally:\n",
    "        MultiHeadAttention.use_sdpa = prev_state\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    use_sdpa = True\n",
    "\n",
    "    def __init__(self, n_state: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.query = Linear(n_state, n_state)\n",
    "        self.key = Linear(n_state, n_state, bias=False)\n",
    "        self.value = Linear(n_state, n_state)\n",
    "        self.out = Linear(n_state, n_state)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        xa: Optional[Tensor] = None,\n",
    "        mask: Optional[Tensor] = None,\n",
    "        kv_cache: Optional[dict] = None,\n",
    "    ):\n",
    "        q = self.query(x)\n",
    "\n",
    "        if kv_cache is None or xa is None or self.key not in kv_cache:\n",
    "            # hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;\n",
    "            # otherwise, perform key/value projections for self- or cross-attention as usual.\n",
    "            k = self.key(x if xa is None else xa)\n",
    "            v = self.value(x if xa is None else xa)\n",
    "        else:\n",
    "            # for cross-attention, calculate keys and values once and reuse in subsequent calls.\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "        return self.out(wv), qk\n",
    "\n",
    "    def qkv_attention(\n",
    "        self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        n_batch, n_ctx, n_state = q.shape\n",
    "        scale = (n_state // self.n_head) ** -0.25\n",
    "        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        if SDPA_AVAILABLE and MultiHeadAttention.use_sdpa:\n",
    "            a = scaled_dot_product_attention(\n",
    "                q, k, v, is_causal=mask is not None and n_ctx > 1\n",
    "            )\n",
    "            out = a.permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = None\n",
    "        else:\n",
    "            qk = (q * scale) @ (k * scale).transpose(-1, -2)\n",
    "            if mask is not None:\n",
    "                qk = qk + mask[:n_ctx, :n_ctx]\n",
    "            qk = qk.float()\n",
    "\n",
    "            w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "            out = (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = qk.detach()\n",
    "\n",
    "        return out, qk\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(n_state, n_head)\n",
    "        self.attn_ln = LayerNorm(n_state)\n",
    "\n",
    "        self.cross_attn = (\n",
    "            MultiHeadAttention(n_state, n_head) if cross_attention else None\n",
    "        )\n",
    "        self.cross_attn_ln = LayerNorm(n_state) if cross_attention else None\n",
    "\n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp = nn.Sequential(\n",
    "            Linear(n_state, n_mlp), nn.GELU(), Linear(n_mlp, n_state)\n",
    "        )\n",
    "        self.mlp_ln = LayerNorm(n_state)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        xa: Optional[Tensor] = None,\n",
    "        mask: Optional[Tensor] = None,\n",
    "        kv_cache: Optional[dict] = None,\n",
    "    ):\n",
    "        x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]\n",
    "        if self.cross_attn:\n",
    "            x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]\n",
    "        x = x + self.mlp(self.mlp_ln(x))\n",
    "        return x\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "        self.conv2 = Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
    "        self.register_buffer(\"positional_embedding\", sinusoids(n_ctx, n_state))\n",
    "\n",
    "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n",
    "            [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_post = LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "\n",
    "        x = F.gelu(self.conv1(x))\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        assert x.shape[1:] == self.positional_embedding.shape, \"incorrect audio shape\"\n",
    "        x = (x + self.positional_embedding).to(x.dtype)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_post(x)\n",
    "        return x\n",
    "\n",
    "class TextDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_vocab: int, n_ctx: int, n_state: int, n_head: int, n_layer: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_state)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))\n",
    "\n",
    "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n",
    "            [\n",
    "                ResidualAttentionBlock(n_state, n_head, cross_attention=True)\n",
    "                for _ in range(n_layer)\n",
    "            ]\n",
    "        )\n",
    "        self.ln = LayerNorm(n_state)\n",
    "\n",
    "        mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)\n",
    "        self.register_buffer(\"mask\", mask, persistent=False)\n",
    "\n",
    "    def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):\n",
    "\n",
    "        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n",
    "        x = (\n",
    "            self.token_embedding(x)\n",
    "            + self.positional_embedding[offset : offset + x.shape[-1]]\n",
    "        )\n",
    "        x = x.to(xa.dtype)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, xa, mask=self.mask, kv_cache=kv_cache)\n",
    "\n",
    "        x = self.ln(x)\n",
    "        logits = (\n",
    "            x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)\n",
    "        ).float()\n",
    "\n",
    "        return logits\n",
    "\n",
    "class Whisper(nn.Module):\n",
    "    def __init__(self, config: ModelDimensions):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.encoder = AudioEncoder(\n",
    "            self.config.n_mels,\n",
    "            self.config.n_audio_ctx,\n",
    "            self.config.n_audio_state,\n",
    "            self.config.n_audio_head,\n",
    "            self.config.n_audio_layer,\n",
    "        )\n",
    "        self.decoder = TextDecoder(\n",
    "            self.config.n_vocab,\n",
    "            self.config.n_text_ctx,\n",
    "            self.config.n_text_state,\n",
    "            self.config.n_text_head,\n",
    "            self.config.n_text_layer,\n",
    "        )\n",
    "\n",
    "        all_heads = torch.zeros(\n",
    "            self.config.n_text_layer, self.config.n_text_head, dtype=torch.bool\n",
    "        )\n",
    "        all_heads[self.config.n_text_layer // 2 :] = True\n",
    "        self.register_buffer(\"alignment_heads\", all_heads.to_sparse(), persistent=False)\n",
    "\n",
    "    def set_alignment_heads(self, dump: bytes):\n",
    "        array = np.frombuffer(\n",
    "            gzip.decompress(base64.b85decode(dump)), dtype=bool\n",
    "        ).copy()\n",
    "        mask = torch.from_numpy(array).reshape(\n",
    "            self.config.n_text_layer, self.config.n_text_head\n",
    "        )\n",
    "        self.register_buffer(\"alignment_heads\", mask.to_sparse(), persistent=False)\n",
    "\n",
    "    def embed_audio(self, mel: torch.Tensor):\n",
    "        return self.encoder(mel)\n",
    "\n",
    "    def logits(self, tokens: torch.Tensor, audio_features: torch.Tensor):\n",
    "        return self.decoder(tokens, audio_features)\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right(input_ids: torch.Tensor, pad_token_id, decoder_start_token_id) -> torch.Tensor:\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape, dtype=torch.long)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "        shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\"pad_token_id has to be defined.\")\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "        return shifted_input_ids\n",
    "\n",
    "    def forward(self, input_features: torch.Tensor, labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
    "        input_features = input_features.float()\n",
    "        encoded_features = self.encoder(input_features)\n",
    "        if labels is not None:\n",
    "            labels = labels.long()\n",
    "            decoder_input_ids = self.shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
    "        else:\n",
    "            decoder_input_ids = None\n",
    "\n",
    "        logits = self.decoder(decoder_input_ids, encoded_features)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            logits = logits.view(-1, self.config.n_vocab)\n",
    "            labels = labels.view(-1).long()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        return {\"loss\": loss, \"logits\": logits, \"input_features\": encoded_features}\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @property\n",
    "    def is_multilingual(self):\n",
    "        return self.config.n_vocab >= 51865\n",
    "\n",
    "    @property\n",
    "    def num_languages(self):\n",
    "        return self.config.n_vocab - 51765 - int(self.is_multilingual)\n",
    "\n",
    "    def install_kv_cache_hooks(self, cache: Optional[dict] = None):\n",
    "\n",
    "        cache = {**cache} if cache is not None else {}\n",
    "        hooks = []\n",
    "\n",
    "        def save_to_cache(module, _, output):\n",
    "            if module not in cache or output.shape[1] > self.config.n_text_ctx:\n",
    "\n",
    "                cache[module] = output\n",
    "            else:\n",
    "                cache[module] = torch.cat([cache[module], output], dim=1).detach()\n",
    "            return cache[module]\n",
    "\n",
    "        def install_hooks(layer: nn.Module):\n",
    "            if isinstance(layer, MultiHeadAttention):\n",
    "                hooks.append(layer.key.register_forward_hook(save_to_cache))\n",
    "                hooks.append(layer.value.register_forward_hook(save_to_cache))\n",
    "\n",
    "        self.decoder.apply(install_hooks)\n",
    "        return cache, hooks\n",
    "\n",
    "    detect_language = detect_language_function\n",
    "    transcribe = transcribe_function\n",
    "    decode = decode_function\n",
    "\n",
    "\n",
    "    def install_kv_cache_hooks(self, cache: Optional[dict] = None):\n",
    "        cache = {**cache} if cache is not None else {}\n",
    "        hooks = []\n",
    "\n",
    "        def save_to_cache(module, _, output):\n",
    "            if module not in cache or output.shape[1] > self.config.n_text_ctx:\n",
    "                cache[module] = output\n",
    "            else:\n",
    "                cache[module] = torch.cat([cache[module], output], dim=1).detach()\n",
    "            return cache[module]\n",
    "\n",
    "        def install_hooks(layer: nn.Module):\n",
    "            if isinstance(layer, MultiHeadAttention):\n",
    "                hooks.append(layer.key.register_forward_hook(save_to_cache))\n",
    "                hooks.append(layer.value.register_forward_hook(save_to_cache))\n",
    "\n",
    "        self.decoder.apply(install_hooks)\n",
    "        return cache, hooks\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings: torch.nn.Embedding):\n",
    "        self.decoder.token_embedding = new_embeddings\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.decoder.token_embedding\n",
    "\n",
    "    def resize_token_embeddings(self, new_num_tokens: int):\n",
    "        old_embeddings = self.get_input_embeddings()\n",
    "        old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n",
    "        new_embeddings = torch.nn.Embedding(new_num_tokens, old_embedding_dim)\n",
    "    \n",
    "        new_embeddings.weight.data[:old_num_tokens, :] = old_embeddings.weight.data\n",
    "        self.set_input_embeddings(new_embeddings)\n",
    "        self.config.n_vocab = new_num_tokens\n",
    "\n",
    "    def save_pretrained(self, save_directory, safetensor=False):\n",
    "        self.save_pretrained(save_directory)\n",
    "        if safetensor:\n",
    "            safetensor_path = os.path.join(save_directory, \"model.safetensors\")\n",
    "            with safe_open(safetensor_path, framework=\"pt\", mode=\"w\") as f:\n",
    "                for key, value in self.state_dict().items():\n",
    "                    f.set_tensor(key, value)\n",
    "        else:\n",
    "            torch.save(self.state_dict(), os.path.join(save_directory, \"pytorch_model.bin\"))\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, safetensor=False, *model_args, **kwargs):\n",
    "        config = WhisperConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "        model = cls(config, *model_args, **kwargs)\n",
    "        if safetensor:\n",
    "            safetensor_path = f\"{pretrained_model_name_or_path}/model.safetensors\"\n",
    "            with safe_open(safetensor_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "                state_dict = {key: torch.tensor(f.get_tensor(key)) for key in f.keys()}\n",
    "        else:\n",
    "            state_dict = torch.load(os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin\"), map_location=\"cpu\")\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        return model\n",
    "\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        return {'input_features': input_ids}\n",
    "\n",
    "    def _prepare_decoder_input_ids_for_generation(self, batch_size, decoder_start_token_id=None, bos_token_id=None):\n",
    "        return torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * self.config.decoder_start_token_id\n",
    "\n",
    "    def can_generate(self):\n",
    "        return True\n",
    "    \n",
    "    def generate(self, inputs, **kwargs):\n",
    "        encoder_outputs = self.encoder(inputs)\n",
    "        decoder_input_ids = torch.zeros((inputs.size(0), 1), dtype=torch.long, device=inputs.device)\n",
    "        outputs = self.decoder(decoder_input_ids, encoder_outputs)\n",
    "        return outputs.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "log_dir = os.path.join('./output/logs/', datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "old_model = \"openai/whisper-small\"\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(old_model)#, sampling_rate=16000, n_fft=1024, hop_length=256, n_mels=80)\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(old_model)\n",
    "proccesor = WhisperProcessor.from_pretrained(old_model, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "\n",
    "config = WhisperConfig(\n",
    "    n_mels=80,\n",
    "    n_audio_ctx=1500,\n",
    "    n_audio_state=768,\n",
    "    n_audio_head=12,\n",
    "    n_audio_layer=12,\n",
    "    n_vocab=51865,\n",
    "    n_text_ctx=448,\n",
    "    n_text_state=768,\n",
    "    n_text_head=12,\n",
    "    n_text_layer=12,\n",
    "    )\n",
    "\n",
    "model = Whisper(config).cuda()\n",
    "\n",
    "optimizer = transformers.Adafactor(model.parameters(), \n",
    "                                clip_threshold=0.99, \n",
    "                                weight_decay=0.025, \n",
    "                                scale_parameter=True, \n",
    "                                relative_step=False, \n",
    "                                warmup_init=False, \n",
    "                                lr=2.25e-3)\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "def get_adamax_optimizer(model, learning_rate=0.001, weight_decay=0.0):\n",
    "    return Adamax(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=\"D:/proj/datasets/gf_1\", cache_dir = \"D:/hf\")['train'].to_iterable_dataset(num_shards=20).filter(lambda x: len(x['sentence']) > 0).map(lambda x: {\"sentence\": neologdn.normalize(x['sentence'], repeat=1)}).shuffle(seed=42, buffer_size=1000)\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    input_features = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    input_features = torch.nan_to_num(torch.tensor(input_features), nan=0.0, posinf=1.0, neginf=-1.0).tolist()\n",
    "    labels = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    labels = torch.nan_to_num(torch.tensor(labels), nan=0.0, posinf=1.0, neginf=-1.0).tolist()\n",
    "    \n",
    "    batch[\"input_features\"] = input_features\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(prepare_dataset).select_columns([\"input_features\", \"labels\"])\n",
    "test , train = dataset.take(100), dataset.skip(100)\n",
    "\n",
    "metric = evaluate.load(\"cer\")\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    proccesor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.proccesor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.proccesor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        \n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        batch[\"input_features\"] = torch.nan_to_num(batch[\"input_features\"], nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        batch[\"labels\"] = torch.nan_to_num(batch[\"labels\"], nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(proccesor=proccesor, decoder_start_token_id=model.config.decoder_start_token_id)\n",
    "cer_metric = evaluate.load(\"cer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorBoardCallback(TrainerCallback):\n",
    "    def __init__(self, tb_writer):\n",
    "        self.tb_writer = tb_writer\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            for key, value in logs.items():\n",
    "                self.tb_writer.add_scalar(key, value, state.global_step)\n",
    "            if 'predictions' in logs and 'label_ids' in logs:\n",
    "                cer = compute_cer(logs['predictions'], logs['label_ids'])\n",
    "                self.tb_writer.add_scalar(\"cer\", cer, state.global_step)\n",
    "\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "        return control\n",
    "    \n",
    "class ShuffleCallback(TrainerCallback):\n",
    "    def on_epoch_begin(self, args, state, control, train_dataloader, **kwargs):\n",
    "        if isinstance(train_dataloader.dataset, IterableDatasetShard):\n",
    "            pass  \n",
    "        elif isinstance(train_dataloader.dataset, IterableDataset):\n",
    "            train_dataloader.dataset.shuffle()       \n",
    "            train_dataloader.dataset.set_epoch(train_dataloader.dataset.epoch + 1)\n",
    "\n",
    "class GradientClippingCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        torch.nn.utils.clip_grad_norm_(kwargs[\"model\"].parameters(), max_norm=1.0)\n",
    "\n",
    "class PredictionLogger(TrainerCallback):\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        if state.global_step % args.logging_steps == 0:\n",
    "            eval_loss = kwargs.get('metrics', {}).get('eval_loss', None)\n",
    "            if eval_loss is not None:\n",
    "                all_predictions = kwargs['metrics']['predictions']\n",
    "                all_labels = kwargs['metrics']['label_ids']\n",
    "                tokenizer = kwargs['tokenizer']\n",
    "\n",
    "                sample_indices = range(min(1, len(all_predictions)))\n",
    "                for idx in sample_indices:\n",
    "                    pred_str = tokenizer.decode(all_predictions[idx], skip_special_tokens=True)\n",
    "                    label_str = tokenizer.decode(all_labels[idx], skip_special_tokens=True)\n",
    "                    print(f\"Evaluation Loss: {eval_loss:.4f}\")\n",
    "                    print(f\"Evaluation Sample {idx}: Prediction: {pred_str}, Label: {label_str}\")\n",
    "\n",
    "class ModelFreezingCallback(TrainerCallback):\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        for name, param in kwargs['model'].named_parameters():\n",
    "            if 'layer.0' in name:  # Change this to match the layers you want to freeze\n",
    "                param.requires_grad = False\n",
    "                print(f\"Freezing layer: {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cer(predictions, label_ids):\n",
    "    pred_str = proccesor.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    label_str = proccesor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    return 100 * cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    wakati = MeCab.Tagger(\"-Owakati\")   \n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = proccesor.tokenizer.pad_token_id\n",
    "    \n",
    "    pred_str = proccesor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = proccesor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    cer = 100 * cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    pred_flat = np.argmax(pred_ids, axis=2).flatten()\n",
    "    labels_flat = label_ids.flatten()\n",
    "    mask = labels_flat != proccesor.tokenizer.pad_token_id\n",
    "\n",
    "    accuracy = accuracy_score(labels_flat[mask], pred_flat[mask])\n",
    "    precision = precision_score(labels_flat[mask], pred_flat[mask], average='weighted')\n",
    "    recall = recall_score(labels_flat[mask], pred_flat[mask], average='weighted')\n",
    "    f1 = f1_score(labels_flat[mask], pred_flat[mask], average='weighted')\n",
    "    \n",
    "    return {\n",
    "        \"cer\": cer,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "tb_writer = SummaryWriter(log_dir=log_dir)\n",
    "tb_callback = CustomTensorBoardCallback(tb_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = \"./output\"\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=log_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=1, \n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2.4e-5,\n",
    "    warmup_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=1000,\n",
    "    tf32=True,\n",
    "    bf16=True,\n",
    "    save_steps=1000,\n",
    "    logging_steps=10,\n",
    "    logging_dir=log_dir+\"logs_trainer/\",\n",
    "    logging_strategy=\"steps\",\n",
    "    report_to=[\"tensorboard\"],\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    hub_private_repo=True,\n",
    "    metric_for_best_model=\"cer\",\n",
    "    predict_with_generate=False,\n",
    "    greater_is_better=False,\n",
    "    generation_max_length=128,\n",
    "    optim=\"adafactor\",\n",
    "    weight_decay=0.0025,\n",
    "    disable_tqdm=False,\n",
    "    save_total_limit=2,  \n",
    "    # torch_empty_cache_steps=1,\n",
    ")\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "train = train.shuffle(seed=42)\n",
    "test = test.shuffle(seed=42)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=proccesor.feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, scheduler), # optimizers=(get_adamax_optimizer, None)\n",
    "    callbacks=[CustomTensorBoardCallback(tb_writer), GradientClippingCallback, PredictionLogger]#, ModelFreezingCallback, SavePeftModelCallback, ShuffleCallback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate()\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "tb_writer.close()\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
