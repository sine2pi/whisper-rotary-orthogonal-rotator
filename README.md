Dynamic Base Adjustment
Self-Adjusting Parameters: The model dynamically adjusts the base parameter in response to training loss, optimizing positional embeddings in real-time. This adaptive mechanism enhances the model's ability to fine-tune itself during training, ensuring better performance and efficiency.

RotaryEmbeddingWithRotation
Orthogonally Initialized Rotation Matrix: This component combines rotary embeddings with an orthogonally initialized rotation matrix, providing robust and stable positional embeddings. This novel approach enhances the modelâ€™s capacity to represent positional information effectively.

LearnedSinusoidalEmbeddings
Learned Sinusoidal Embeddings with Checkpointing: This unique integration of sinusoidal embeddings with optional checkpointing helps manage memory efficiently during training while maintaining stable embedding magnitudes through L2 normalization.

MultiHeadAttention
Dynamic Positional Bias: Supports rotary embeddings and includes relative positional bias, capturing dependencies effectively. The attention mechanism is finely tuned with a dynamically adjustable base parameter, providing flexibility and precision.

HybridAttention
Combining Local and Global Attention: This component leverages both local and global attention mechanisms, ensuring that the model captures both fine-grained and broad context. The sliding window approach for local attention enhances its ability to process long sequences efficiently.

DynamicConvAttention
Integrating Convolution and Attention: This component enriches feature representation by combining convolutional layers with attention mechanisms, enabling the model to extract local context while attending to global information simultaneously.

Model Components
LayerNorm: Custom normalization with gamma and beta parameters.

Linear: Custom linear layer with batch normalization and various activation functions.

Conv1d: Custom 1D convolution layer with Kaiming initialization.

RotaryEmbeddingWithRotation: Orthogonally initialized rotary embeddings with dynamic base adjustment.

LearnedSinusoidalEmbeddings: Sinusoidal embeddings with optional checkpointing and L2 normalization.

MultiHeadAttention: Dynamic positional bias with rotary embeddings and optional caching.

ResidualAttentionBlock: Integrates self and cross-attention with GELU-activated MLP.

AudioEncoder: Convolutional layers with learned sinusoidal embeddings and rotary embeddings.

TextDecoder: Token embeddings with rotary embeddings and cross-attention.

DynamicConvAttention: Combines convolution and attention for enriched feature extraction.

HybridAttention: Merges local and global attention mechanisms using sliding window and multi-head attention.



{'train_runtime': 40.2698, 'train_samples_per_second': 2.483, 'train_steps_per_second': 2.483, 'train_loss': 23.43394317626953, 'epoch': 1.0}

