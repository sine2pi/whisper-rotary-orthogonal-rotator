{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, random, math, time\n",
    "import math\n",
    "import logging, warnings, csv, base64, gzip\n",
    "import json, datetime, numpy as np, torch, torch.nn as nn\n",
    "import torch.optim as optim, torch.nn.functional as F, torchaudio\n",
    "import torchaudio.transforms as transforms, torch.utils.checkpoint as checkpoint\n",
    "import torch.utils.tensorboard as tensorboard, torch.optim.lr_scheduler as lr_scheduler\n",
    "import transformers, neologdn, evaluate, MeCab, deepl, logging, datasets, tqdm, whisper\n",
    "import transformers.utils.logging\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Subset\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "from torch.profiler import profile, ProfilerActivity, record_function\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import amp, Tensor\n",
    "from torch.optim import Adamax\n",
    "import logging\n",
    "from transformers import (\n",
    "    logging,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    PretrainedConfig,\n",
    "    GenerationConfig,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizerFast,\n",
    "    WhisperTokenizer,\n",
    "    WhisperModel,\n",
    "    WhisperConfig,\n",
    "    Adafactor,\n",
    "    TrainerCallback,\n",
    "    logging\n",
    ")\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "warnings.warn = lambda *args, **kwargs: None\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "checkpointing_args = {\"reentrant\": False}\n",
    "\n",
    "try:\n",
    "    from torch.nn.functional import scaled_dot_product_attention\n",
    "    SDPA_AVAILABLE = True\n",
    "except (ImportError, RuntimeError, OSError):\n",
    "    scaled_dot_product_attention = None\n",
    "    SDPA_AVAILABLE = False\n",
    "\n",
    "from whisper.decoding import decode as decode_function\n",
    "from whisper.decoding import detect_language as detect_language_function\n",
    "from whisper.transcribe import transcribe as transcribe_function\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "transformers.utils.logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    def forward(self, x) -> Tensor:\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "class Linear(nn.Linear):\n",
    "    def __init__(self, *args, dropout_rate: float = 0.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.reset_parameters()\n",
    "        self.dropout = nn.Dropout(dropout_rate)  \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x) -> Tensor:\n",
    "        weight = self.weight.to(x.dtype)\n",
    "        bias = None if self.bias is None else self.bias.to(x.dtype)\n",
    "        x = F.linear(x, weight, bias)  \n",
    "        return self.dropout(x) \n",
    "    \n",
    "class Conv1d(nn.Conv1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, nonlinearity='relu')\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def _conv_forward(self, x, weight, bias) -> Tensor:\n",
    "        weight = self.weight.to(x.dtype)\n",
    "        bias = None if self.bias is None else self.bias.to(x.dtype)\n",
    "        return super()._conv_forward(x, weight, bias)\n",
    "\n",
    "class RotationLayer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.rotation_matrix = nn.Parameter(torch.eye(embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        rotated_x = torch.matmul(x, self.rotation_matrix)\n",
    "        return rotated_x\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.orthogonal_(self.rotation_matrix)\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, base=10000):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def rotate_queries_or_keys(self, x):\n",
    "        sinusoid_inp = torch.einsum('i , j -> i j', torch.arange(x.shape[1], device=x.device), self.inv_freq) \n",
    "        sin = sinusoid_inp.sin()[None, :, None, :] \n",
    "        cos = sinusoid_inp.cos()[None, :, None, :]\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        x = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "        return x\n",
    "\n",
    "class SinusoidalFeatures:\n",
    "    def __init__(self, n_ctx, n_state):\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_state = n_state\n",
    "        self.features = self.sinusoidal_features(n_ctx, n_state)\n",
    "\n",
    "    @staticmethod\n",
    "    def sinusoidal_features(n_ctx, n_state):\n",
    "        position = torch.arange(0, n_ctx, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, n_state, 2).float() * -(math.log(10000.0) / n_state))\n",
    "        features = torch.zeros(n_ctx, n_state)\n",
    "        features[:, 0::2] = torch.sin(position * div_term)\n",
    "        features[:, 1::2] = torch.cos(position * div_term)\n",
    "        return features\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.features\n",
    "\n",
    "class LearnedSinusoidalEmbeddings(nn.Module): \n",
    "    def __init__(self, n_ctx, n_state, gradient_checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_state = n_state\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "\n",
    "        sinusoidal_embeddings = SinusoidalFeatures(n_ctx, n_state)()\n",
    "        self.positional_embeddings = nn.Parameter(sinusoidal_embeddings)\n",
    "\n",
    "    def forward(self, positions):\n",
    "        if self.gradient_checkpointing:\n",
    "            position_embeddings = checkpoint.checkpoint(lambda x: self.positional_embeddings[x], positions)\n",
    "        else:\n",
    "            position_embeddings = self.positional_embeddings[positions]\n",
    "\n",
    "        position_embeddings = F.normalize(position_embeddings, p=2, dim=-1)  \n",
    "        return position_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasedCrossAttention(nn.Module):\n",
    "    def __init__(self, n_state, n_head, dropout_rate=0.1, group_norm=False):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.n_state = n_state\n",
    "        self.head_dim = n_state // n_head\n",
    "\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(n_head, self.head_dim))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.norm = nn.GroupNorm(num_groups=16, num_channels=n_state) if group_norm else nn.LayerNorm(n_state) \n",
    "\n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        q = self.query(q).view(q.shape[0], q.shape[1], self.n_head, self.head_dim)\n",
    "        k = self.key(k).view(k.shape[0], k.shape[1], self.n_head, self.head_dim)\n",
    "        v = self.value(v).view(v.shape[0], v.shape[1], self.n_head, self.head_dim)\n",
    "\n",
    "        qk = (q @ k.transpose(-2, -1)) / self.head_dim ** 0.5 + self.bias\n",
    "        if mask is not None:\n",
    "            qk += mask\n",
    "\n",
    "        w = F.softmax(qk, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "\n",
    "        out = (w @ v).permute(0, 2, 1, 3).reshape(q.shape[0], q.shape[1], -1)\n",
    "        out = self.norm(self.out(out) + q.view(q.shape[0], q.shape[1], -1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedMemory(nn.Module):\n",
    "    def __init__(self, n_state, n_head, memory_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = n_state // n_head\n",
    "        self.memory_size = memory_size\n",
    "\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.memory = nn.Parameter(torch.randn(memory_size, n_state))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x).view(x.size(0), x.size(1), self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(self.memory).view(self.memory_size, self.n_head, self.head_dim).transpose(1, 0)\n",
    "        v = self.value(self.memory).view(self.memory_size, self.n_head, self.head_dim).transpose(1, 0)\n",
    "\n",
    "        qk = torch.einsum('bhqd,hnd->bhnq', q, k) / self.head_dim ** 0.5\n",
    "        w = F.softmax(qk, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "\n",
    "        out = torch.einsum('bhnq,hnd->bhqd', w, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(x.size(0), -1, self.n_head * self.head_dim)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DynamicConvAttention(nn.Module):\n",
    "    def __init__(self, n_state, n_head, kernel_size=3, dropout_rate=0.1, group_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_state = n_state\n",
    "        self.n_head = n_head\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv1d(n_state, n_state, kernel_size, padding=kernel_size//2, groups=n_head)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.query = Linear(n_state, n_state)\n",
    "        self.key = Linear(n_state, n_state, bias=False)\n",
    "        self.value = Linear(n_state, n_state)\n",
    "        self.out = Linear(n_state, n_state)\n",
    "\n",
    "        self.norm = nn.GroupNorm(num_groups=16, num_channels=n_state) if group_norm else nn.LayerNorm(n_state)       \n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        if embed_dim != self.n_state:\n",
    "            raise ValueError(f\"Expected embed_dim of {self.n_state}, but got {embed_dim}\")\n",
    "\n",
    "        x = x.permute(0, 2, 1) \n",
    "        conv_out = self.conv(x)\n",
    "        conv_out = conv_out.permute(0, 2, 1) \n",
    "        conv_out = self.norm(conv_out)\n",
    "        conv_out = self.dropout(conv_out)\n",
    "        return self.out(conv_out) + x.permute(0, 2, 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HybridAttention(nn.Module):\n",
    "    def __init__(self, n_state, n_head, window_size=1, dropout_rate=0.1, group_norm=False):\n",
    "        super().__init__()\n",
    "        self.local_attn = nn.MultiheadAttention(n_state, n_head, dropout=dropout_rate)\n",
    "        self.global_attn = nn.MultiheadAttention(n_state, n_head, dropout=dropout_rate)\n",
    "        self.group_norm = group_norm\n",
    "        self.ln_local = GroupNorm(num_groups=16, num_channels=n_state) if group_norm else LayerNorm(n_state)\n",
    "        self.ln_global = GroupNorm(num_groups=16, num_channels=n_state) if group_norm else LayerNorm(n_state)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x_local = self.ln_local(x)\n",
    "        x_global = self.ln_global(x)\n",
    "        x_local = x_local.permute(1, 0, 2)\n",
    "        x_global = x_global.permute(1, 0, 2)\n",
    "        local_out = self.sliding_window_attention(x_local)\n",
    "        global_out, _ = self.global_attn(x_global, x_global, x_global)\n",
    "        combined_out = local_out + global_out\n",
    "        combined_out = combined_out.permute(1, 0, 2)\n",
    "        return self.dropout(combined_out)\n",
    "\n",
    "    def sliding_window_attention(self, x):\n",
    "        seq_len, batch_size, n_state = x.size()\n",
    "        window_size = min(self.window_size, max(1, seq_len // 4)) \n",
    "        output = torch.zeros_like(x, device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        for i in range(0, seq_len, window_size):\n",
    "            end = min(i + window_size, seq_len)\n",
    "            query = x[i:end, :, :]\n",
    "            start = max(0, i - window_size)\n",
    "            key = x[start:end, :, :]\n",
    "            value = x[start:end, :, :]\n",
    "            attn_output, _ = self.local_attn(query, key, value)\n",
    "            output[i:end, :, :] = attn_output[:end - i, :, :]\n",
    "            \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GroupNorm(nn.Module): \n",
    "    def __init__(self, num_groups, num_channels, eps=1e-6):\n",
    "        super(GroupNorm, self).__init__()\n",
    "        self.num_groups = num_groups\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(num_channels))\n",
    "        self.b = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.scale = nn.Parameter(torch.ones(num_channels))\n",
    "\n",
    "        self.group_norm = nn.GroupNorm(num_groups, num_channels, eps=eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.group_norm(x)\n",
    "        norm_x = torch.norm(x, dim=-1, keepdim=True)\n",
    "        rms_x = x / norm_x\n",
    "        scaled_x = rms_x * self.scale.view(1, -1, 1)\n",
    "        x = self.g.view(1, -1, 1) * scaled_x + self.b.view(1, -1, 1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    sdpa = True\n",
    "\n",
    "    def __init__(self, n_state, n_head, dropout_rate=0.01, gradient_checkpointing=False, group_norm=False, use_rotation_dynamics=False, use_dynamic_attention_integration=False, hybrid_attention=False, dynamic_conv=False, biased_attention=False, augmented_memory=False):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.n_state = n_state\n",
    "        self.head_dim = n_state // n_head\n",
    "\n",
    "        self.query = Linear(n_state, n_state)\n",
    "        self.key = Linear(n_state, n_state, bias=False)\n",
    "        self.value = Linear(n_state, n_state)\n",
    "        self.out = Linear(n_state, n_state)\n",
    "\n",
    "\n",
    "\n",
    "        self.use_rotation_dynamics = use_rotation_dynamics\n",
    "        self.use_dynamic_attention_integration = use_dynamic_attention_integration\n",
    "        self.hybrid_attention = hybrid_attention\n",
    "        self.dynamic_conv = dynamic_conv\n",
    "        self.biased_attention = biased_attention\n",
    "        self.augmented_memory = augmented_memory\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "        self.group_norm = group_norm\n",
    "\n",
    "        self.rotary_emb = RotaryEmbedding(dim=self.head_dim)\n",
    "        self.rotation_layer = RotationLayer(self.head_dim) \n",
    "        # self.temperature = nn.Parameter(torch.ones(1) * (self.head_dim ** -0.5))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.attn_ln = GroupNorm(num_groups=16, num_channels=n_state) if group_norm else nn.LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x, xa = None, mask = None, kv_cache = None):\n",
    "        x_norm = self.attn_ln(x)\n",
    "\n",
    "        q = self.query(x_norm)\n",
    "        k = self.key(x_norm if xa is None else xa)\n",
    "        v = self.value(x_norm if xa is None else xa)\n",
    "\n",
    "        if kv_cache is not None and self.key in kv_cache:\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "\n",
    "        q = q.view(q.shape[0], q.shape[1], self.n_head, -1)\n",
    "        k = k.view(k.shape[0], k.shape[1], self.n_head, -1)\n",
    "\n",
    "        q = self.rotary_emb.rotate_queries_or_keys(q)\n",
    "        k = self.rotary_emb.rotate_queries_or_keys(k)\n",
    "        q = self.rotation_layer(q)\n",
    "        k = self.rotation_layer(k)\n",
    "\n",
    "        q = q.view(q.shape[0], q.shape[1], -1)\n",
    "        k = k.view(k.shape[0], k.shape[1], -1)\n",
    "\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "\n",
    "        return self.out(wv) + x, qk\n",
    "\n",
    "    def qkv_attention(self, q, k, v, mask = None):\n",
    "        n_batch, n_ctx, n_state = q.shape\n",
    "        scale = 1.0##### quick fix needs work\n",
    "\n",
    "        q = q.view(n_batch, n_ctx, self.n_head, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = k.view(n_batch, k.shape[1], self.n_head, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = v.view(n_batch, v.shape[1], self.n_head, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        if SDPA_AVAILABLE and MultiHeadAttention.sdpa:\n",
    "            a = scaled_dot_product_attention(q, k, v, is_causal=mask is not None and n_ctx > 1)\n",
    "            out = a.permute(0, 2, 1, 3).reshape(n_batch, n_ctx, n_state)\n",
    "            qk = None\n",
    "        else:\n",
    "            qk = (q * scale) @ (k.transpose(-2, -1) * scale)\n",
    "            if mask is not None:\n",
    "                qk += mask[:n_ctx, :n_ctx]\n",
    "            qk = qk.float()\n",
    "\n",
    "            w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "            w = self.dropout(w)\n",
    "            out = (w @ v).permute(0, 2, 1, 3).reshape(n_batch, n_ctx, n_state)\n",
    "            qk = qk.detach()\n",
    "\n",
    "        return out, qk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state, n_head, dynamic_conv = False, hybrid_attention = False,\n",
    "                 biased_attention = False, augmented_memory = False,\n",
    "                 cross_attention = False, dropout_rate=0.1, gradient_checkpointing=False, group_norm=False, \n",
    "                 window_size=1, use_rotation_dynamics=False, use_dynamic_attention_integration=False):\n",
    "        super().__init__()\n",
    "        self.query = Linear(n_state, n_state)\n",
    "        self.key = Linear(n_state, n_state, bias=False)\n",
    "        self.value = Linear(n_state, n_state)\n",
    "        self.out = Linear(n_state, n_state)\n",
    "\n",
    "        self.use_rotation_dynamics = use_rotation_dynamics\n",
    "        self.use_dynamic_attention_integration = use_dynamic_attention_integration\n",
    "        self.hybrid_attention = hybrid_attention\n",
    "        self.dynamic_conv = dynamic_conv\n",
    "        self.biased_attention = biased_attention\n",
    "        self.augmented_memory = augmented_memory\n",
    "\n",
    "        self.attention_mechanisms = nn.ModuleList([MultiHeadAttention(n_state, n_head, dropout_rate=dropout_rate, gradient_checkpointing=gradient_checkpointing, group_norm=group_norm)])\n",
    "        \n",
    "        if self.hybrid_attention:\n",
    "            self.attention_mechanisms.append(HybridAttention(n_state, n_head, window_size=window_size, dropout_rate=dropout_rate, group_norm=group_norm))\n",
    "        if self.dynamic_conv:\n",
    "            self.attention_mechanisms.append(DynamicConvAttention(n_state, n_head, dropout_rate=dropout_rate))\n",
    "        if self.biased_attention:\n",
    "            self.attention_mechanisms.append(BiasedCrossAttention(n_state, n_head, dropout_rate=dropout_rate))\n",
    "        if self.augmented_memory:\n",
    "            self.attention_mechanisms.append(AugmentedMemory(n_state, memory_size=512, n_head=n_head, dropout_rate=dropout_rate))\n",
    "\n",
    "        self.attn_scores = nn.Parameter(torch.randn(len(self.attention_mechanisms)))\n",
    "        self.attn_ln = GroupNorm(num_groups=16, num_channels=n_state) if group_norm else nn.LayerNorm(n_state)\n",
    "        self.cross_attention = cross_attention\n",
    "        if self.cross_attention:\n",
    "            self.cross_attn = MultiHeadAttention(n_state, n_head, dropout_rate=dropout_rate, gradient_checkpointing=gradient_checkpointing, group_norm=group_norm)\n",
    "            self.cross_attn_ln = GroupNorm(num_groups=16, num_channels=n_state) if group_norm else nn.LayerNorm(n_state)\n",
    "\n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp = nn.Sequential(\n",
    "            Linear(n_state, n_mlp),\n",
    "            GroupNorm(num_groups=16, num_channels=n_mlp) if group_norm else nn.LayerNorm(n_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            Linear(n_mlp, n_state)\n",
    "        )\n",
    "        self.mlp_ln = GroupNorm(num_groups=16, num_channels=n_state) if group_norm else nn.LayerNorm(n_state)\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "\n",
    "    def forward(self, x, xa = None, mask = None, kv_cache = None, \n",
    "                biased_attention=False, augmented_memory=False):\n",
    "        global printed_attention_mechanisms_once\n",
    "        if not printed_attention_mechanisms_once:\n",
    "            print(\"Attention mechanisms being used:\")\n",
    "            for attn in self.attention_mechanisms:\n",
    "                print(f\"- {type(attn).__name__}\")\n",
    "            printed_attention_mechanisms_once = True\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(xa if xa is not None else x)\n",
    "        v = self.value(xa if xa is not None else x)\n",
    "\n",
    "        attn_input = self.attn_ln(x)\n",
    "        attn_outputs = []\n",
    "\n",
    "        for attn in self.attention_mechanisms:\n",
    "            if isinstance(attn, BiasedCrossAttention) and not biased_attention:\n",
    "                continue\n",
    "            if isinstance(attn, AugmentedMemory) and not augmented_memory:\n",
    "                continue\n",
    "\n",
    "            if self.gradient_checkpointing:\n",
    "                output = checkpoint.checkpoint(attn, attn_input, k, v, mask) if isinstance(attn, BiasedCrossAttention) else checkpoint.checkpoint(attn, attn_input)\n",
    "            else:\n",
    "                output = attn(attn_input, k, v, mask) if isinstance(attn, BiasedCrossAttention) else attn(attn_input)\n",
    "\n",
    "            attn_outputs.append(output if isinstance(output, Tensor) else output[0])\n",
    "\n",
    "        if not attn_outputs:\n",
    "            raise ValueError(\"No attention mechanisms are enabled. Ensure at least one attention mechanism is active.\")\n",
    "\n",
    "        if self.use_dynamic_attention_integration:\n",
    "            attn_outputs = torch.stack(attn_outputs, dim=0)\n",
    "            attn_scores = F.softmax(self.attn_scores[:len(attn_outputs)], dim=0)  # Match scores to the number of attention mechanisms\n",
    "            weighted_attn_output = torch.einsum('i,ijkm->ijkm', attn_scores, attn_outputs).sum(dim=0)\n",
    "            attn_out = x + weighted_attn_output\n",
    "        else:\n",
    "            attn_out = x + attn_outputs[0]\n",
    "\n",
    "        if self.cross_attention and xa is not None:\n",
    "            cross_attn_input = self.cross_attn_ln(attn_out)\n",
    "            if self.gradient_checkpointing:\n",
    "                attn_out = attn_out + checkpoint.checkpoint(self.cross_attn, cross_attn_input, xa, kv_cache)[0]\n",
    "            else:\n",
    "                attn_out = attn_out + self.cross_attn(cross_attn_input, xa, kv_cache=kv_cache)[0]\n",
    "\n",
    "        mlp_input = self.mlp_ln(attn_out)\n",
    "        if self.gradient_checkpointing:\n",
    "            mlp_out = attn_out + checkpoint.checkpoint(self.mlp, mlp_input)\n",
    "        else:\n",
    "            mlp_out = attn_out + self.mlp(mlp_input)\n",
    "\n",
    "        return mlp_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEncoder(nn.Module):\n",
    "    main_input_name = \"input_features\"\n",
    "\n",
    "    def __init__(self, window_size, n_mels, n_ctx, n_state, n_head, n_layer,\n",
    "                 dropout_rate=0.1, gradient_checkpointing=False, dynamic_conv = False, hybrid_attention = False, biased_attention = False, augmented_memory = False, group_norm=False,\n",
    "                 use_rotation_dynamics=False, use_dynamic_attention_integration=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.query = Linear(n_state, n_state)\n",
    "        self.key = Linear(n_state, n_state, bias=False)\n",
    "        self.value = Linear(n_state, n_state)\n",
    "        self.out = Linear(n_state, n_state)\n",
    "\n",
    "        self.conv1 = Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "        self.conv2 = Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "        self.group_norm = group_norm\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResidualAttentionBlock(\n",
    "                n_state, n_head,\n",
    "                window_size=window_size,\n",
    "                hybrid_attention=hybrid_attention,                \n",
    "                dynamic_conv=dynamic_conv,\n",
    "                biased_attention=biased_attention,\n",
    "                augmented_memory=augmented_memory,\n",
    "                dropout_rate=dropout_rate,\n",
    "                gradient_checkpointing=gradient_checkpointing,\n",
    "                group_norm=group_norm,\n",
    "                use_rotation_dynamics=use_rotation_dynamics,  \n",
    "                use_dynamic_attention_integration=use_dynamic_attention_integration \n",
    "            )\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "\n",
    "        if self.group_norm:\n",
    "            self.ln_post = GroupNorm(num_groups=16, num_channels=n_state)\n",
    "        else:\n",
    "            self.ln_post = LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = F.gelu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            if self.gradient_checkpointing:\n",
    "                x = checkpoint.checkpoint(block, x)\n",
    "            else:\n",
    "                x = block(x)\n",
    "\n",
    "        x = self.ln_post(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDecoder(nn.Module):\n",
    "    def __init__(self, n_vocab, n_ctx, n_state, n_head, n_layer,\n",
    "                 dropout_rate=0.1, gradient_checkpointing=False, dynamic_conv=False, hybrid_attention=False, biased_attention=False, augmented_memory=False, group_norm=False,\n",
    "                 use_rotation_dynamics=False, use_dynamic_attention_integration=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_state)\n",
    "        self.positional_embedding = LearnedSinusoidalEmbeddings(n_ctx, n_state, gradient_checkpointing=gradient_checkpointing)\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "        self.group_norm = group_norm\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.rotary_emb = RotaryEmbedding(dim=n_state // n_head)  \n",
    "        self.rotation_layer = RotationLayer(n_state // n_head) \n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResidualAttentionBlock(\n",
    "                n_state, n_head,\n",
    "                hybrid_attention=hybrid_attention,\n",
    "                dynamic_conv=dynamic_conv,\n",
    "                biased_attention=biased_attention,\n",
    "                augmented_memory=augmented_memory,\n",
    "                cross_attention=True,\n",
    "                dropout_rate=dropout_rate,\n",
    "                gradient_checkpointing=gradient_checkpointing,\n",
    "                group_norm=group_norm,\n",
    "                use_rotation_dynamics=use_rotation_dynamics, \n",
    "                use_dynamic_attention_integration=use_dynamic_attention_integration  \n",
    "            )\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "\n",
    "        if self.group_norm:\n",
    "            self.ln_post = GroupNorm(num_groups=16, num_channels=n_state)\n",
    "        else:\n",
    "            self.ln_post = LayerNorm(n_state)\n",
    "\n",
    "        mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)\n",
    "        self.register_buffer('mask', mask, persistent=False)\n",
    "\n",
    "    def forward(self, x, xa, kv_cache=None):\n",
    "        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n",
    "        positions = torch.arange(x.shape[1], device=x.device) + offset\n",
    "        pos_emb = self.positional_embedding(positions).unsqueeze(0)\n",
    "\n",
    "        x = self.token_embedding(x) + pos_emb\n",
    "        x = x.to(xa.dtype)\n",
    "\n",
    "        batch_size, seq_length, embedding_dim = x.shape\n",
    "        num_heads = self.n_head\n",
    "        head_dim = embedding_dim // num_heads\n",
    "        x = x.view(batch_size, seq_length, num_heads, head_dim)\n",
    "\n",
    "        x = self.rotary_emb.rotate_queries_or_keys(x)\n",
    "        x = self.rotation_layer(x)\n",
    "\n",
    "        x = x.view(batch_size, seq_length, embedding_dim)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            if self.gradient_checkpointing:\n",
    "                x = checkpoint.checkpoint(block, x, xa, self.mask, kv_cache)\n",
    "            else:\n",
    "                x = block(x, xa, self.mask, kv_cache)\n",
    "\n",
    "        x = self.ln_post(x)\n",
    "        logits = (x @ self.token_embedding.weight.to(x.dtype).T).float()\n",
    "        if kv_cache is not None:\n",
    "            for layer, cache in self.kv_cache_layers.items():\n",
    "                kv_cache[layer] = cache\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperConfig(PretrainedConfig):\n",
    "    model_type = \"whisper\"\n",
    "    keys_to_ignore_at_inference = []\n",
    "    attribute_map = {}\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation_dropout = kwargs.get(\"activation_dropout\", 0.0)\n",
    "        self.activation_function = kwargs.get(\"activation_function\", \"gelu\")\n",
    "        self.architectures = kwargs.get(\"architectures\", [\"WhisperForConditionalGeneration\"])\n",
    "        self.attention_dropout = kwargs.get(\"attention_dropout\", 0.0)\n",
    "        self.begin_suppress_tokens = kwargs.get(\"begin_suppress_tokens\", [220, 50257])\n",
    "        self.bos_token_id = kwargs.get(\"bos_token_id\", 50257)\n",
    "        self.d_model = kwargs.get(\"d_model\", 1024)\n",
    "        self.decoder_attention_heads = kwargs.get(\"decoder_attention_heads\", 16)\n",
    "        self.decoder_ffn_dim = kwargs.get(\"decoder_ffn_dim\", 4096)\n",
    "        self.decoder_layerdrop = kwargs.get(\"decoder_layerdrop\", 0.0)\n",
    "        self.decoder_layers = kwargs.get(\"decoder_layers\", 24)\n",
    "        self.decoder_start_token_id = kwargs.get(\"decoder_start_token_id\", 50258)\n",
    "        self.dropout = kwargs.get(\"dropout\", 0.0)\n",
    "        self.encoder_attention_heads = kwargs.get(\"encoder_attention_heads\", 16)\n",
    "        self.encoder_ffn_dim = kwargs.get(\"encoder_ffn_dim\", 4096)\n",
    "        self.encoder_layerdrop = kwargs.get(\"encoder_layerdrop\", 0.0)\n",
    "        self.encoder_layers = kwargs.get(\"encoder_layers\", 24)\n",
    "        self.eos_token_id = kwargs.get(\"eos_token_id\", 50257)\n",
    "        self.forced_decoder_ids = kwargs.get(\"forced_decoder_ids\", [[1, 50259], [2, 50359], [3, 50363]])\n",
    "        self.init_std = kwargs.get(\"init_std\", 0.02)\n",
    "        self.is_encoder_decoder = kwargs.get(\"is_encoder_decoder\", True)\n",
    "        self.max_length = kwargs.get(\"max_length\", 448)\n",
    "        self.max_source_positions = kwargs.get(\"max_source_positions\", 1500)\n",
    "        self.max_target_positions = kwargs.get(\"max_target_positions\", 448)\n",
    "        self.num_hidden_layers = kwargs.get(\"num_hidden_layers\", 24)\n",
    "        self.num_mel_bins = kwargs.get(\"num_mel_bins\", 80)\n",
    "        self.pad_token_id = kwargs.get(\"pad_token_id\", 50257)\n",
    "        self.scale_embedding = kwargs.get(\"scale_embedding\", False)\n",
    "        self.suppress_tokens = kwargs.get(\"suppress_tokens\", [\n",
    "            1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873,\n",
    "            893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585,\n",
    "            6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553,\n",
    "            16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865,\n",
    "            42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362\n",
    "        ])\n",
    "        self.torch_dtype = kwargs.get(\"torch_dtype\", \"float32\")\n",
    "        self.transformers_version = kwargs.get(\"transformers_version\", \"4.27.0.dev0\")\n",
    "        self.cache = kwargs.get(\"cache\", True)\n",
    "        self.vocab_size = kwargs.get(\"vocab_size\", 51865)\n",
    "\n",
    "        # Compatibility mappings\n",
    "        self.n_vocab = self.vocab_size\n",
    "        self.n_mels = self.num_mel_bins\n",
    "        self.n_audio_ctx = self.max_source_positions\n",
    "        self.n_audio_state = self.d_model\n",
    "        self.n_audio_head = self.encoder_attention_heads\n",
    "        self.n_audio_layer = self.encoder_layers\n",
    "        self.n_text_ctx = self.max_target_positions\n",
    "        self.n_text_state = self.d_model\n",
    "        self.n_text_head = self.decoder_attention_heads\n",
    "        self.n_text_layer = self.decoder_layers\n",
    "        self.dropout_rate = self.dropout\n",
    "\n",
    "        super().__init__(\n",
    "            pad_token_id=kwargs.get(\"pad_token_id\", 50256),\n",
    "            bos_token_id=kwargs.get(\"bos_token_id\", 50256),\n",
    "            eos_token_id=kwargs.get(\"eos_token_id\", 50256),\n",
    "            is_encoder_decoder=kwargs.get(\"is_encoder_decoder\", True),\n",
    "            decoder_start_token_id=kwargs.get(\"decoder_start_token_id\", 50256),\n",
    "            suppress_tokens=kwargs.get(\"suppress_tokens\", None),\n",
    "            begin_suppress_tokens=kwargs.get(\"begin_suppress_tokens\", [220, 50256]),\n",
    "            **kwargs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Whisper(nn.Module):\n",
    "    def __init__(self, config: WhisperConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.generation_config = GenerationConfig()\n",
    "        self.encoder = AudioEncoder(\n",
    "            self.config.window_size,\n",
    "            self.config.n_mels,\n",
    "            self.config.n_audio_ctx,\n",
    "            self.config.n_audio_state,\n",
    "            self.config.n_audio_head,\n",
    "            self.config.n_audio_layer,\n",
    "            self.config.dropout_rate,\n",
    "            self.config.gradient_checkpointing,\n",
    "            self.config.dynamic_conv,\n",
    "            self.config.hybrid_attention,\n",
    "            self.config.biased_attention,\n",
    "            self.config.augmented_memory,\n",
    "            self.config.group_norm,\n",
    "            self.config.use_rotation_dynamics,\n",
    "            self.config.use_dynamic_attention_integration \n",
    "        )\n",
    "        self.decoder = TextDecoder(\n",
    "            self.config.n_vocab,\n",
    "            self.config.n_text_ctx,\n",
    "            self.config.n_text_state,\n",
    "            self.config.n_text_head,\n",
    "            self.config.n_text_layer,\n",
    "            self.config.dropout_rate,\n",
    "            self.config.dynamic_conv,\n",
    "            self.config.hybrid_attention,\n",
    "            self.config.biased_attention,\n",
    "            self.config.augmented_memory,\n",
    "            self.config.group_norm,\n",
    "            self.config.use_rotation_dynamics, \n",
    "            self.config.use_dynamic_attention_integration\n",
    "        )\n",
    "\n",
    "        all_heads = torch.zeros(\n",
    "            self.config.n_text_layer, self.config.n_text_head, dtype=torch.bool\n",
    "        )\n",
    "        all_heads[self.config.n_text_layer // 2:] = True\n",
    "        self.register_buffer('alignment_heads', all_heads.to_sparse(), persistent=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_tokens_right(input_ids: torch.Tensor, pad_token_id, decoder_start_token_id) -> torch.Tensor:\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "        shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\"pad_token_id has to be defined.\")\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "        return shifted_input_ids\n",
    "\n",
    "    def forward(self, input_features: torch.Tensor, labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
    "        encoded_features = self.encoder(input_features)\n",
    "        \n",
    "        if labels is not None:\n",
    "            decoder_input_ids = self.shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
    "        else:\n",
    "            decoder_input_ids = None\n",
    "        \n",
    "        logits = self.decoder(decoder_input_ids, encoded_features)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            logits = logits.view(-1, self.config.n_vocab)  \n",
    "            labels = labels.view(-1).long()  \n",
    "            loss = loss_fct(logits, labels)  \n",
    "        return {\"loss\": loss, \"logits\": logits, \"input_features\": encoded_features}\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @property\n",
    "    def is_multilingual(self):\n",
    "        return self.config.n_vocab >= len(tokenizer)\n",
    "\n",
    "    @property\n",
    "    def num_languages(self):\n",
    "        return self.config.n_vocab - (len(tokenizer)-100)\n",
    "\n",
    "    def set_alignment_heads(self, dump: bytes):\n",
    "        array = np.frombuffer(\n",
    "            gzip.decompress(base64.b85decode(dump)), dtype=bool\n",
    "        ).copy()\n",
    "        mask = torch.from_numpy(array).reshape(\n",
    "            self.config.n_text_layer, self.config.n_text_head\n",
    "        )\n",
    "        self.register_buffer('alignment_heads', mask.to_sparse(), persistent=False)\n",
    "\n",
    "    def embed_aud(self, input_features: torch.Tensor):\n",
    "        return self.encoder(input_features)\n",
    "\n",
    "    def logits(self, input_ids: torch.Tensor, input_features: torch.Tensor):\n",
    "        return self.decoder(input_ids, input_features)\n",
    "\n",
    "    def install_kv_cache_hooks(self, cache = None):\n",
    "        cache = {**cache} if cache is not None else {}\n",
    "        hooks = []\n",
    "\n",
    "        def save_to_cache(module, _, output):\n",
    "            if module not in cache or output.shape[1] > self.config.n_text_ctx:\n",
    "                cache[module] = output\n",
    "            else:\n",
    "                cache[module] = torch.cat([cache[module], output], dim=1).detach()\n",
    "            return cache[module]\n",
    "\n",
    "        def install_hooks(layer: nn.Module):\n",
    "            if isinstance(layer, MultiHeadAttention):\n",
    "                hooks.append(layer.key.register_forward_hook(save_to_cache))\n",
    "                hooks.append(layer.value.register_forward_hook(save_to_cache))\n",
    "\n",
    "        self.decoder.apply(install_hooks)\n",
    "        return cache, hooks\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings: torch.nn.Embedding):\n",
    "        self.decoder.token_embedding = new_embeddings\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.decoder.token_embedding\n",
    "\n",
    "    def resize_token_embeddings(self, new_num_tokens):\n",
    "        old_embeddings = self.get_input_embeddings()\n",
    "        old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n",
    "        new_embeddings = torch.nn.Embedding(new_num_tokens, old_embedding_dim)\n",
    "    \n",
    "        new_embeddings.weight.data[:old_num_tokens, :] = old_embeddings.weight.data\n",
    "        self.set_input_embeddings(new_embeddings)\n",
    "        self.config.n_vocab = new_num_tokens\n",
    "\n",
    "    detect_language = detect_language_function\n",
    "    transcribe = transcribe_function\n",
    "    decode = decode_function\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, ignore_mismatched_sizes=False, **kwargs):\n",
    "        config = WhisperConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "        model = cls(config, **kwargs)\n",
    "        state_dict = torch.load(os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin\"), map_location=\"cpu\")\n",
    "        \n",
    "        if ignore_mismatched_sizes:\n",
    "            model_state_dict = model.state_dict()\n",
    "            for key in state_dict.keys():\n",
    "                if key in model_state_dict and state_dict[key].size() != model_state_dict[key].size():\n",
    "                    print(f\"Skipping loading of {key} due to size mismatch\")\n",
    "                    state_dict[key] = model_state_dict[key]\n",
    "        \n",
    "        model.load_state_dict(state_dict, strict=not ignore_mismatched_sizes)\n",
    "        return model\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        return {'input_features': input_ids}\n",
    "\n",
    "    def _prepare_decoder_input_ids_for_generation(self, batch_size, decoder_start_token_id=None, bos_token_id=None):\n",
    "        return torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * self.config.decoder_start_token_id\n",
    "\n",
    "    def can_generate(self):\n",
    "        return True\n",
    "    \n",
    "    def generate(self, inputs, **kwargs):\n",
    "        encoder_outputs = self.encoder(inputs)\n",
    "        decoder_input_ids = torch.zeros((inputs.size(0), 1), dtype=torch.long, device=inputs.device)\n",
    "        outputs = self.decoder(decoder_input_ids, encoder_outputs)\n",
    "        return outputs.argmax(dim=-1)\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        self.config.save_pretrained(save_directory)\n",
    "        torch.save(self.state_dict(), os.path.join(save_directory, \"pytorch_model.bin\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "log_dir = os.path.join('./output/logs', datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "old_model = \"D:/proj3/checkpoints/whisper3\"\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(old_model) \n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(old_model)\n",
    "proccesor = WhisperProcessor.from_pretrained(old_model, tokenizer=tokenizer, feature_extractor=feature_extractor, local_files_only=True)\n",
    "\n",
    "\n",
    "config = WhisperConfig(\n",
    "    n_mels=80,\n",
    "    n_audio_ctx=1500,\n",
    "    n_audio_state=768, \n",
    "    n_audio_head=12, \n",
    "    n_audio_layer=6, \n",
    "    n_vocab=len(tokenizer),#55116,\n",
    "    n_text_ctx=448,\n",
    "    n_text_state=768, \n",
    "    n_text_head=12, \n",
    "    n_text_layer=6,\n",
    "    dropout_rate=0.01,\n",
    "    max_source_positions=1500,\n",
    "    window_size=40,\n",
    "    gradient_checkpointing=False,\n",
    "    hybrid_attention=False,\n",
    "    dynamic_conv=False,\n",
    "    biased_attention=False,\n",
    "    augmented_memory=False,\n",
    "    group_norm=False,\n",
    "    use_rotation_dynamics=True,\n",
    "    use_dynamic_attention_integration=False,\n",
    ")\n",
    "\n",
    "model = Whisper(config).cuda()\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "optimizer = transformers.Adafactor(model.parameters(), \n",
    "                                clip_threshold=0.99, \n",
    "                                weight_decay=0.025, \n",
    "                                scale_parameter=True, \n",
    "                                relative_step=False, \n",
    "                                warmup_init=False, \n",
    "                                lr=2.25e-3)\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "def get_adamax_optimizer(model, learning_rate=0.001, weight_decay=0.0):\n",
    "    return Adamax(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "global printed_attention_mechanisms_once\n",
    "printed_attention_mechanisms_once = False\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=\"D:/proj/datasets/gf_1\", cache_dir = \"D:/hf\")['train'].to_iterable_dataset(num_shards=20).filter(lambda x: len(x['sentence']) > 0).map(lambda x: {\"sentence\": neologdn.normalize(x['sentence'], repeat=1)}).shuffle(seed=42, buffer_size=1000)\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(prepare_dataset).select_columns([\"input_features\", \"labels\"])\n",
    "test , train = dataset.take(100), dataset.skip(100)\n",
    "\n",
    "metric = evaluate.load(\"cer\")\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    proccesor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.proccesor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.proccesor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(proccesor=proccesor, decoder_start_token_id=model.config.decoder_start_token_id)\n",
    "\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "class CustomTensorBoardCallback(TrainerCallback):\n",
    "    def __init__(self, tb_writer):\n",
    "        self.tb_writer = tb_writer\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            for key, value in logs.items():\n",
    "                self.tb_writer.add_scalar(key, value, state.global_step)\n",
    "            if 'predictions' in logs and 'label_ids' in logs:\n",
    "                cer = compute_cer(logs['predictions'], logs['label_ids'])\n",
    "                self.tb_writer.add_scalar(\"cer\", cer, state.global_step)\n",
    "\n",
    "def compute_cer(predictions, label_ids):\n",
    "    pred_str = proccesor.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    label_str = proccesor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    return 100 * cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    wakati = MeCab.Tagger(\"-Owakati\")   \n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = proccesor.tokenizer.pad_token_id\n",
    "    \n",
    "    pred_str = proccesor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = proccesor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    cer = 100 * cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    pred_flat = np.argmax(pred_ids, axis=2).flatten()\n",
    "    labels_flat = label_ids.flatten()\n",
    "    mask = labels_flat != proccesor.tokenizer.pad_token_id\n",
    "\n",
    "    accuracy = accuracy_score(labels_flat[mask], pred_flat[mask])\n",
    "    precision = precision_score(labels_flat[mask], pred_flat[mask], average='weighted')\n",
    "    recall = recall_score(labels_flat[mask], pred_flat[mask], average='weighted')\n",
    "    f1 = f1_score(labels_flat[mask], pred_flat[mask], average='weighted')\n",
    "    \n",
    "    return {\n",
    "        \"cer\": cer,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "tb_writer = SummaryWriter(log_dir=log_dir)\n",
    "tb_callback = CustomTensorBoardCallback(tb_writer)\n",
    "\n",
    "out = log_dir\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=out,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=1, \n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2.4e-5,\n",
    "    warmup_steps=5,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=100,\n",
    "    tf32=True,\n",
    "    bf16=True,\n",
    "    save_steps=1000,\n",
    "    logging_steps=5,\n",
    "    logging_dir=out+\"/log_2\",\n",
    "    logging_strategy=\"steps\",\n",
    "    report_to=[\"tensorboard\"],\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    hub_private_repo=True,\n",
    "    metric_for_best_model=\"cer\",\n",
    "    predict_with_generate=False,\n",
    "    greater_is_better=False,\n",
    "    generation_max_length=128,\n",
    "    optim=\"adafactor\",\n",
    "    weight_decay=0.0025,\n",
    "    disable_tqdm=False,\n",
    "    save_total_limit=2,  \n",
    "    # torch_empty_cache_steps=1,\n",
    "    gradient_checkpointing_kwargs={\"reentrant\": False},\n",
    "    # max_grad_norm=0.9,\n",
    ")\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "train = train.shuffle(seed=42)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=proccesor.feature_extractor,\n",
    "    # optimizers=(optimizer, scheduler), # optimizers=(get_adamax_optimizer, None)\n",
    "    callbacks=[tb_callback]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
