{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, random, math, time\n",
    "import math\n",
    "import logging, warnings, csv, base64, gzip\n",
    "import json, datetime, numpy as np, torch, torch.nn as nn\n",
    "import torch.optim as optim, torch.nn.functional as F, torchaudio\n",
    "import torchaudio.transforms as transforms, torch.utils.checkpoint as checkpoint\n",
    "import torch.utils.tensorboard as tensorboard, torch.optim.lr_scheduler as lr_scheduler\n",
    "import transformers, neologdn, evaluate, MeCab, deepl, logging, datasets, tqdm, whisper\n",
    "import transformers.utils.logging\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Subset, IterableDataset\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "from torch.profiler import profile, ProfilerActivity, record_function\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import amp, Tensor\n",
    "from torch.optim import Adamax\n",
    "import logging\n",
    "from safetensors import safe_open\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from transformers.trainer_pt_utils import IterableDatasetShard\n",
    "from transformers.trainer_utils import is_main_process\n",
    "from transformers.trainer_pt_utils import find_batch_size, get_parameter_names\n",
    "from transformers import (\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    "    logging,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    PretrainedConfig,\n",
    "    GenerationConfig,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizerFast,\n",
    "    WhisperTokenizer,\n",
    "    WhisperModel,\n",
    "    WhisperConfig,\n",
    "    Adafactor,\n",
    "    TrainerCallback,\n",
    "    logging\n",
    ")\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "warnings.warn = lambda *args, **kwargs: None\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "checkpointing_args = {\"reentrant\": False}\n",
    "\n",
    "try:\n",
    "    from torch.nn.functional import scaled_dot_product_attention\n",
    "    SDPA_AVAILABLE = True\n",
    "except (ImportError, RuntimeError, OSError):\n",
    "    scaled_dot_product_attention = None\n",
    "    SDPA_AVAILABLE = False\n",
    "\n",
    "from whisper.decoding import decode as decode_function\n",
    "from whisper.decoding import detect_language as detect_language_function\n",
    "from whisper.transcribe import transcribe as transcribe_function\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "transformers.utils.logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasedCrossAttention(nn.Module):\n",
    "    def __init__(self, n_state, n_head, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.n_state = n_state\n",
    "        self.head_dim = n_state // n_head\n",
    "\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(n_head, 1, self.head_dim))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.norm = LayerNorm(n_state)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size, seq_length, _ = q.size()\n",
    "\n",
    "        q = self.query(q).view(batch_size, seq_length, self.n_head, self.head_dim)\n",
    "        k = self.key(k).view(batch_size, seq_length, self.n_head, self.head_dim)\n",
    "        v = self.value(v).view(batch_size, seq_length, self.n_head, self.head_dim)\n",
    "\n",
    "        qk = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5) + self.bias\n",
    "        if mask is not None:\n",
    "            qk = qk.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        w = F.softmax(qk, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "\n",
    "        out = (w @ v).transpose(1, 2).contiguous().view(batch_size, seq_length, -1)\n",
    "        out = self.norm(self.out(out) + q.view(batch_size, seq_length, -1))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d(nn.Conv1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, nonlinearity='relu')\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def _conv_forward(self, x, weight, bias) -> Tensor:\n",
    "        weight = self.weight.to(x.dtype)\n",
    "        bias = None if self.bias is None else self.bias.to(x.dtype)\n",
    "        return super()._conv_forward(x, weight, bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedSinusoidalEmbeddings(nn.Module):\n",
    "    def __init__(self, n_ctx, n_state, checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_state = n_state\n",
    "        self.checkpointing = checkpointing\n",
    "\n",
    "        position = torch.arange(0, n_ctx, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, n_state, 2).float() * -(math.log(10000.0) / n_state))\n",
    "        features = torch.zeros(n_ctx, n_state)\n",
    "        features[:, 0::2] = torch.sin(position * div_term)\n",
    "        features[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('sinusoidal_features', features)\n",
    "\n",
    "        self.positional_embeddings = nn.Parameter(self.sinusoidal_features.clone())\n",
    "\n",
    "    def forward(self, positions):\n",
    "        if self.checkpointing:\n",
    "            position_embeddings = checkpoint(lambda x: self.positional_embeddings[x], positions)\n",
    "        else:\n",
    "            position_embeddings = self.positional_embeddings[positions]\n",
    "\n",
    "        position_embeddings = torch.nn.functional.normalize(position_embeddings, p=2, dim=-1)  \n",
    "        return position_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicConvAttention(nn.Module):\n",
    "    def __init__(self, n_state, n_head, kernel_size=3, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.n_state = n_state\n",
    "        self.n_head = n_head\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.conv = nn.Conv1d(n_state, n_state, kernel_size, padding=kernel_size // 2, groups=n_head)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.out_proj = nn.Linear(n_state, n_state)\n",
    "\n",
    "        self.norm = LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        if embed_dim != self.n_state:\n",
    "            raise ValueError(f\"Expected embed_dim of {self.n_state}, but got {embed_dim}\")\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        conv_out = self.conv(x)\n",
    "        conv_out = conv_out.permute(0, 2, 1)\n",
    "        conv_out = self.norm(conv_out)\n",
    "        conv_out = self.dropout(conv_out)\n",
    "\n",
    "        attention_out = F.softmax(torch.matmul(q, k.transpose(-2, -1)) / (self.n_state ** 0.5), dim=-1)\n",
    "        attention_out = torch.matmul(attention_out, v)\n",
    "        \n",
    "        combined_out = conv_out + attention_out\n",
    "        combined_out = self.norm(combined_out)\n",
    "        \n",
    "        return self.out_proj(self.dropout(combined_out)) + x.permute(0, 2, 1)\n",
    "\n",
    "class HybridAttention(nn.Module):\n",
    "    def __init__(self, n_state, n_head, window_size=1, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.local_attn = nn.MultiheadAttention(n_state, n_head, dropout=dropout_rate)\n",
    "        self.global_attn = nn.MultiheadAttention(n_state, n_head, dropout=dropout_rate)\n",
    "        self.ln_local = LayerNorm(n_state)\n",
    "        self.ln_global = LayerNorm(n_state)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x_local = self.ln_local(x)\n",
    "        x_global = self.ln_global(x)\n",
    "        x_local = x_local.permute(1, 0, 2)\n",
    "        x_global = x_global.permute(1, 0, 2)\n",
    "        local_out = self.sliding_window_attention(x_local)\n",
    "        global_out, _ = self.global_attn(x_global, x_global, x_global)\n",
    "        combined_out = local_out + global_out\n",
    "        combined_out = combined_out.permute(1, 0, 2)\n",
    "        return self.dropout(combined_out)\n",
    "\n",
    "    def sliding_window_attention(self, x):\n",
    "        seq_len, batch_size, n_state = x.size()\n",
    "        window_size = min(self.window_size, max(1, seq_len // 4))\n",
    "        output = torch.zeros_like(x, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        for i in range(0, seq_len, window_size):\n",
    "            end = min(i + window_size, seq_len)\n",
    "            query = x[i:end, :, :]\n",
    "            start = max(0, i - window_size)\n",
    "            key = x[start:end, :, :]\n",
    "            value = x[start:end, :, :]\n",
    "            attn_output, _ = self.local_attn(query, key, value)\n",
    "            output[i:end, :, :] = attn_output[:end - i, :, :]\n",
    "\n",
    "        return output\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        x = (x - mean) / (std + self.eps)\n",
    "        return self.gamma * x + self.beta\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, dropout_rate: float = 0.01, use_batchnorm: bool = True, activation: str = 'relu'):\n",
    "        super(Linear, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.activation = activation\n",
    "\n",
    "        if self.use_batchnorm:\n",
    "            self.batchnorm = nn.BatchNorm1d(out_features)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.linear.weight, nonlinearity=self.activation)\n",
    "        if self.linear.bias is not None:\n",
    "            nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = x.view(-1, x.size(-1))  \n",
    "        x = self.linear(x)\n",
    "\n",
    "        if self.use_batchnorm:\n",
    "            x = self.batchnorm(x)\n",
    "\n",
    "        x = self.apply_activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(batch_size, seq_len, -1)  \n",
    "        \n",
    "        return x\n",
    "\n",
    "    def apply_activation(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif self.activation == 'tanh':\n",
    "            return torch.tanh(x)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported activation function: {self.activation}')\n",
    "\n",
    "def sinusoids(length, channels, max_timescale=10000):\n",
    "    \"\"\"Returns sinusoids for positional embedding\"\"\"\n",
    "    assert channels % 2 == 0\n",
    "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
    "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    use_sdpa = True\n",
    "\n",
    "    def __init__(self, n_state: int, n_head: int, base: int = 10000, checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.query = nn.Linear(n_state, n_state)\n",
    "        self.key = nn.Linear(n_state, n_state, bias=False)\n",
    "        self.value = nn.Linear(n_state, n_state)\n",
    "        self.out = nn.Linear(n_state, n_state)\n",
    "        self.h_dim = n_state // n_head\n",
    "        self.checkpointing=checkpointing\n",
    "\n",
    "        self.rotation_matrix = nn.Parameter(torch.empty(self.h_dim, self.h_dim))\n",
    "        nn.init.orthogonal_(self.rotation_matrix)\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.h_dim, 2).float() / self.h_dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def rotate_queries_or_keys(self, x):\n",
    "        sinusoid_inp = torch.einsum('i , j -> i j', torch.arange(x.shape[1], device=x.device), self.inv_freq)\n",
    "        sin = sinusoid_inp.sin()[None, :, None, :]\n",
    "        cos = sinusoid_inp.cos()[None, :, None, :]\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        x = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor, xa: Optional[torch.Tensor] = None, mask: Optional[torch.Tensor] = None, kv_cache: Optional[dict] = None):\n",
    "        q = self.query(x)\n",
    "\n",
    "        if kv_cache is None or xa is None or self.key not in kv_cache:\n",
    "            k = self.key(x if xa is None else xa)\n",
    "            v = self.value(x if xa is None else xa)\n",
    "        else:\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "\n",
    "        q = q.view(q.shape[0], q.shape[1], self.n_head, -1)\n",
    "        k = k.view(k.shape[0], k.shape[1], self.n_head, -1)\n",
    "\n",
    "        q = self.rotate_queries_or_keys(q)\n",
    "        k = self.rotate_queries_or_keys(k)\n",
    "        q = torch.matmul(q, self.rotation_matrix)\n",
    "        k = torch.matmul(k, self.rotation_matrix)\n",
    "\n",
    "        q = q.view(q.shape[0], q.shape[1], -1)\n",
    "        k = k.view(k.shape[0], k.shape[1], -1)\n",
    "\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "        return self.out(wv), qk\n",
    "\n",
    "    def qkv_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        n_batch, n_ctx, n_state = q.shape\n",
    "        scale = (n_state // self.n_head) ** -0.25\n",
    "        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        if SDPA_AVAILABLE and MultiHeadAttention.use_sdpa:\n",
    "            a = scaled_dot_product_attention(q, k, v, is_causal=mask is not None and n_ctx > 1)\n",
    "            out = a.permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = None\n",
    "        else:\n",
    "            qk = (q * scale) @ (k * scale).transpose(-1, -2)\n",
    "            if mask is not None:\n",
    "                qk = qk + mask[:n_ctx, :n_ctx]\n",
    "            qk = qk.float()\n",
    "\n",
    "            w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "            out = (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = qk.detach()\n",
    "\n",
    "        return out, qk\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False, checkpointing=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(n_state, n_head)\n",
    "        self.attn_ln = LayerNorm(n_state)\n",
    "        self.checkpointing=checkpointing\n",
    "        \n",
    "        self.cross_attn = (\n",
    "            MultiHeadAttention(n_state, n_head) if cross_attention else None\n",
    "        )\n",
    "        self.cross_attn_ln = LayerNorm(n_state) if cross_attention else None\n",
    "\n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp = nn.Sequential(\n",
    "            Linear(n_state, n_mlp), nn.GELU(), Linear(n_mlp, n_state)\n",
    "        )\n",
    "        self.mlp_ln = LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, xa: Optional[torch.Tensor] = None, mask: Optional[torch.Tensor] = None, kv_cache: Optional[dict] = None):\n",
    "        residual = x\n",
    "        x = self.attn_ln(x)\n",
    "        x = residual + self.attn(x, mask=mask, kv_cache=kv_cache)[0]\n",
    "\n",
    "        if self.cross_attn:\n",
    "            residual = x\n",
    "            x = self.cross_attn_ln(x)\n",
    "            x = residual + self.cross_attn(x, xa, kv_cache=kv_cache)[0]\n",
    "\n",
    "        residual = x\n",
    "        x = self.mlp_ln(x)\n",
    "        x = residual + self.mlp(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbeddingWithRotation(nn.Module):\n",
    "    def __init__(self, n_state, n_head, base=10000):\n",
    "        super().__init__()\n",
    "        self.n_state = n_state\n",
    "        self.n_head = n_head\n",
    "        self.h_dim = n_state // n_head\n",
    "        \n",
    "        self.rotation_matrix = nn.Parameter(torch.eye(self.h_dim))\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, self.h_dim, 2).float() / self.h_dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.orthogonal_(self.rotation_matrix)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 3:\n",
    "            batch_size, seq_len, n_state = x.size()\n",
    "        elif x.dim() == 4:\n",
    "            batch_size, seq_len, n_head, h_dim = x.size()\n",
    "            n_state = n_head * h_dim\n",
    "            x = x.view(batch_size, seq_len, n_state)\n",
    "        else:\n",
    "            raise ValueError(f\"Expected input tensor to be 3D or 4D, but got {x.dim()}D\")\n",
    "\n",
    "        if n_state != self.n_state:\n",
    "            raise ValueError(f\"Expected n_state of {self.n_state}, but got {n_state}\")\n",
    "\n",
    "        x = x.reshape(batch_size, seq_len, self.n_head, self.h_dim)\n",
    "\n",
    "        x = x.reshape(-1, self.h_dim)\n",
    "        rotated_x = torch.matmul(x, self.rotation_matrix)\n",
    "        rotated_x = rotated_x.reshape(batch_size, seq_len, self.n_head, self.h_dim)\n",
    "\n",
    "        sinusoid_inp = torch.einsum('i, j -> i j', torch.arange(seq_len, device=x.device), self.inv_freq)\n",
    "        sin = sinusoid_inp.sin()[None, :, None, :]\n",
    "        cos = sinusoid_inp.cos()[None, :, None, :]\n",
    "        x1, x2 = rotated_x[..., ::2], rotated_x[..., 1::2]\n",
    "        rotated_x = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "        \n",
    "        rotated_x = rotated_x.reshape(batch_size, seq_len, self.n_state)\n",
    "        return rotated_x\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class LearnedSinusoidalEmbeddings(nn.Module):\n",
    "    def __init__(self, n_ctx, n_state, checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_state = n_state\n",
    "        self.checkpointing = checkpointing\n",
    "\n",
    "        position = torch.arange(0, n_ctx, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, n_state, 2).float() * -(math.log(10000.0) / n_state))\n",
    "        features = torch.zeros(n_ctx, n_state)\n",
    "        features[:, 0::2] = torch.sin(position * div_term)\n",
    "        features[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('sinusoidal_features', features)\n",
    "\n",
    "        self.positional_embeddings = nn.Parameter(self.sinusoidal_features.clone())\n",
    "\n",
    "    def forward(self, positions):\n",
    "        if self.checkpointing:\n",
    "            position_embeddings = checkpoint(lambda x: self.positional_embeddings[x], positions)\n",
    "        else:\n",
    "            position_embeddings = self.positional_embeddings[positions]\n",
    "\n",
    "        position_embeddings = torch.nn.functional.normalize(position_embeddings, p=2, dim=-1)  \n",
    "        return position_embeddings\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int, checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
    "        self.positional_embedding = LearnedSinusoidalEmbeddings(n_ctx, n_state, checkpointing=checkpointing)\n",
    "        self.rotor_layer = RotaryEmbeddingWithRotation(n_state, n_head)\n",
    "        self.checkpointing = checkpointing\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [ResidualAttentionBlock(n_state, n_head, checkpointing=checkpointing) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_post = LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = F.gelu(self.conv1(x))\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = self.rotor_layer(x)\n",
    "        \n",
    "        pos_emb = self.positional_embedding(torch.arange(x.size(1), device=x.device)).unsqueeze(0)\n",
    "        x = x + pos_emb\n",
    "\n",
    "        for block in self.blocks:\n",
    "            if self.checkpointing:\n",
    "                x = checkpoint(block, x)\n",
    "            else:\n",
    "                x = block(x)\n",
    "\n",
    "        x = self.ln_post(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDecoder(nn.Module):\n",
    "    def __init__(self, n_vocab, n_ctx, n_state, n_head, n_layer, checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_state)\n",
    "        self.positional_embedding = LearnedSinusoidalEmbeddings(n_ctx, n_state, checkpointing=checkpointing)\n",
    "        self.rotor_layer = RotaryEmbeddingWithRotation(n_state, n_head)\n",
    "        self.checkpointing = checkpointing\n",
    "        self.n_head = n_head\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResidualAttentionBlock(n_state, n_head, cross_attention=True, checkpointing=checkpointing)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln = LayerNorm(n_state)\n",
    "        mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)\n",
    "        self.register_buffer(\"mask\", mask, persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, xa: torch.Tensor, kv_cache: Optional[dict] = None):\n",
    "        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n",
    "        positions = torch.arange(x.shape[1], device=x.device) + offset\n",
    "        pos_emb = self.positional_embedding(positions).unsqueeze(0)\n",
    "\n",
    "        x = self.token_embedding(x) + pos_emb\n",
    "        x = x.to(xa.dtype)\n",
    "\n",
    "        batch_size, seq_length, embedding_dim = x.shape\n",
    "        num_heads = self.n_head\n",
    "        head_dim = embedding_dim // num_heads\n",
    "        x = x.view(batch_size, seq_length, num_heads, head_dim)\n",
    "\n",
    "        x = self.rotor_layer(x)\n",
    "        x = x.view(batch_size, seq_length, embedding_dim)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            if self.checkpointing:\n",
    "                x = checkpoint(block, x, xa, self.mask, kv_cache)\n",
    "            else:\n",
    "                x = block(x, xa, self.mask, kv_cache)\n",
    "\n",
    "        x = self.ln(x)\n",
    "        logits = (x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)).float()\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Whisper(nn.Module):\n",
    "    def __init__(self, config: WhisperConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.encoder = AudioEncoder(\n",
    "            self.config.n_mels,\n",
    "            self.config.n_audio_ctx,\n",
    "            self.config.n_audio_state,\n",
    "            self.config.n_audio_head,\n",
    "            self.config.n_audio_layer,\n",
    "            self.config.checkpointing,\n",
    "        )\n",
    "        self.decoder = TextDecoder(\n",
    "            self.config.n_vocab,\n",
    "            self.config.n_text_ctx,\n",
    "            self.config.n_text_state,\n",
    "            self.config.n_text_head,\n",
    "            self.config.n_text_layer,\n",
    "            self.config.checkpointing,\n",
    "        )\n",
    "\n",
    "        all_heads = torch.zeros(\n",
    "            self.config.n_text_layer, self.config.n_text_head, dtype=torch.bool\n",
    "        )\n",
    "        all_heads[self.config.n_text_layer // 2 :] = True\n",
    "        self.register_buffer(\"alignment_heads\", all_heads.to_sparse(), persistent=False)\n",
    "\n",
    "    def set_alignment_heads(self, dump: bytes):\n",
    "        array = np.frombuffer(\n",
    "            gzip.decompress(base64.b85decode(dump)), dtype=bool\n",
    "        ).copy()\n",
    "        mask = torch.from_numpy(array).reshape(\n",
    "            self.config.n_text_layer, self.config.n_text_head\n",
    "        )\n",
    "        self.register_buffer(\"alignment_heads\", mask.to_sparse(), persistent=False)\n",
    "\n",
    "    def embed_audio(self, mel: torch.Tensor):\n",
    "        return self.encoder(mel)\n",
    "\n",
    "    def logits(self, tokens: torch.Tensor, input_features: torch.Tensor):\n",
    "        return self.decoder(tokens, input_features)\n",
    "    \n",
    "    @staticmethod\n",
    "    def shift_tokens_right(input_ids: torch.Tensor, pad_token_id, decoder_start_token_id) -> torch.Tensor:\n",
    "        shifted_input_ids = input_ids.new_zeros(input_ids.shape, dtype=torch.long)\n",
    "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "        shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\"pad_token_id has to be defined.\")\n",
    "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "        return shifted_input_ids\n",
    "\n",
    "    def forward(self, input_features: torch.Tensor, labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
    "        input_features = input_features.float()\n",
    "        if torch.isnan(input_features).any():\n",
    "            print(\"NaNs detected in input features\")\n",
    "\n",
    "        encoded_features = self.encoder(input_features)\n",
    "        if torch.isnan(encoded_features).any():\n",
    "            print(\"NaNs detected in encoded features\")\n",
    "\n",
    "        if labels is not None:\n",
    "            labels = labels.long()\n",
    "            decoder_input_ids = self.shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
    "            if torch.isnan(decoder_input_ids).any():\n",
    "                print(\"NaNs detected in decoder input IDs\")\n",
    "        else:\n",
    "            decoder_input_ids = None\n",
    "\n",
    "        logits = self.decoder(decoder_input_ids, encoded_features)\n",
    "        if torch.isnan(logits).any():\n",
    "            print(\"NaNs detected in logits\")\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            logits = logits.view(-1, self.config.n_vocab)\n",
    "            labels = labels.view(-1).long()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            if torch.isnan(loss).any():\n",
    "                print(\"NaNs detected in loss- do the NaN dance!\")\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits, \"input_features\": encoded_features}\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @property\n",
    "    def is_multilingual(self):\n",
    "        return self.config.n_vocab >= 51865\n",
    "\n",
    "    @property\n",
    "    def num_languages(self):\n",
    "        return self.config.n_vocab - 51765 - int(self.is_multilingual)\n",
    "\n",
    "    def install_kv_cache_hooks(self, cache: Optional[dict] = None):\n",
    "        cache = {**cache} if cache is not None else {}\n",
    "        hooks = []\n",
    "\n",
    "        def save_to_cache(module, _, output):\n",
    "            if module not in cache or output.shape[1] > self.config.n_text_ctx:\n",
    "                cache[module] = output\n",
    "            else:\n",
    "                cache[module] = torch.cat([cache[module], output], dim=1).detach()\n",
    "            return cache[module]\n",
    "\n",
    "        def install_hooks(layer: nn.Module):\n",
    "            if isinstance(layer, MultiHeadAttention):\n",
    "                hooks.append(layer.key.register_forward_hook(save_to_cache))\n",
    "                hooks.append(layer.value.register_forward_hook(save_to_cache))\n",
    "\n",
    "        self.decoder.apply(install_hooks)\n",
    "        return cache, hooks\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings: torch.nn.Embedding):\n",
    "        self.decoder.token_embedding = new_embeddings\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.decoder.token_embedding\n",
    "\n",
    "    def resize_token_embeddings(self, new_num_tokens: int):\n",
    "        old_embeddings = self.get_input_embeddings()\n",
    "        old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n",
    "        new_embeddings = torch.nn.Embedding(new_num_tokens, old_embedding_dim)\n",
    "    \n",
    "        new_embeddings.weight.data[:old_num_tokens, :] = old_embeddings.weight.data\n",
    "        self.set_input_embeddings(new_embeddings)\n",
    "        self.config.n_vocab = new_num_tokens\n",
    "\n",
    "    detect_language = detect_language_function\n",
    "    transcribe = transcribe_function\n",
    "    decode = decode_function\n",
    "\n",
    "    def save_pretrained(self, save_directory, safetensor=False):\n",
    "        self.config.save_pretrained(save_directory)\n",
    "        if safetensor:\n",
    "            safetensor_path = os.path.join(save_directory, \"model.safetensors\")\n",
    "            with safe_open(safetensor_path, framework=\"pt\", mode=\"w\") as f:\n",
    "                for key, value in self.state_dict().items():\n",
    "                    f.set_tensor(key, value)\n",
    "        else:\n",
    "            torch.save(self.state_dict(), os.path.join(save_directory, \"pytorch_model.bin\"))\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, safetensor=False, *model_args, **kwargs):\n",
    "        config = WhisperConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "        model = cls(config, *model_args, **kwargs)\n",
    "        if safetensor:\n",
    "            safetensor_path = f\"{pretrained_model_name_or_path}/model.safetensors\"\n",
    "            with safe_open(safetensor_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "                state_dict = {key: torch.tensor(f.get_tensor(key)) for key in f.keys()}\n",
    "        else:\n",
    "            state_dict = torch.load(os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin\"), map_location=\"cpu\")\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        return model\n",
    "\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        return {'input_features': input_ids}\n",
    "\n",
    "    def _prepare_decoder_input_ids_for_generation(self, batch_size, decoder_start_token_id=None, bos_token_id=None):\n",
    "        return torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * self.config.decoder_start_token_id\n",
    "\n",
    "    def can_generate(self):\n",
    "        return True\n",
    "    \n",
    "    def generate(self, inputs, **kwargs):\n",
    "        encoder_outputs = self.encoder(inputs)\n",
    "        decoder_input_ids = torch.zeros((inputs.size(0), 1), dtype=torch.long, device=inputs.device)\n",
    "        outputs = self.decoder(decoder_input_ids, encoder_outputs)\n",
    "        return outputs.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461e075e4da0461fb370a9f558f05970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "log_dir = os.path.join('./output/logs/', datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "old_model = \"openai/whisper-small\"\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(old_model)#, sampling_rate=16000, n_fft=1024, hop_length=256, n_mels=80)\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(old_model)\n",
    "proccesor = WhisperProcessor.from_pretrained(old_model, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "\n",
    "config = WhisperConfig(\n",
    "    n_mels=80,\n",
    "    n_audio_ctx=1500,\n",
    "    n_audio_state=1024,\n",
    "    n_audio_head=16,\n",
    "    n_audio_layer=24,\n",
    "    n_vocab=51865,\n",
    "    n_text_ctx=448,\n",
    "    n_text_state=1024,\n",
    "    n_text_head=16,\n",
    "    n_text_layer=20,\n",
    "    checkpointing=True,\n",
    "    )\n",
    "\n",
    "model = Whisper(config).cuda()\n",
    "\n",
    "optimizer = transformers.Adafactor(model.parameters(), \n",
    "                                clip_threshold=0.99, \n",
    "                                weight_decay=0.025, \n",
    "                                scale_parameter=True, \n",
    "                                relative_step=False, \n",
    "                                warmup_init=False, \n",
    "                                lr=2.25e-3)\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "def get_adamax_optimizer(model, learning_rate=0.001, weight_decay=0.0):\n",
    "    return Adamax(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=\"D:/proj/datasets/gf_1\", cache_dir = \"D:/hf\")['train'].to_iterable_dataset(num_shards=20).filter(lambda x: len(x['sentence']) > 0).map(lambda x: {\"sentence\": neologdn.normalize(x['sentence'], repeat=1)}).shuffle(seed=42, buffer_size=1000)\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    input_features = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    input_features = torch.nan_to_num(torch.tensor(input_features), nan=0.0, posinf=1.0, neginf=-1.0).tolist()\n",
    "    labels = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    labels = torch.nan_to_num(torch.tensor(labels), nan=0.0, posinf=1.0, neginf=-1.0).tolist()\n",
    "    \n",
    "    batch[\"input_features\"] = input_features\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(prepare_dataset).select_columns([\"input_features\", \"labels\"])\n",
    "test , train = dataset.take(100), dataset.skip(100)\n",
    "\n",
    "metric = evaluate.load(\"cer\")\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    proccesor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.proccesor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.proccesor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        \n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        batch[\"input_features\"] = torch.nan_to_num(batch[\"input_features\"], nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        batch[\"labels\"] = torch.nan_to_num(batch[\"labels\"], nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(proccesor=proccesor, decoder_start_token_id=model.config.decoder_start_token_id)\n",
    "cer_metric = evaluate.load(\"cer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorBoardCallback(TrainerCallback):\n",
    "    def __init__(self, tb_writer):\n",
    "        self.tb_writer = tb_writer\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            for key, value in logs.items():\n",
    "                self.tb_writer.add_scalar(key, value, state.global_step)\n",
    "            if 'predictions' in logs and 'label_ids' in logs:\n",
    "                cer = compute_cer(logs['predictions'], logs['label_ids'])\n",
    "                self.tb_writer.add_scalar(\"cer\", cer, state.global_step)\n",
    "\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "        return control\n",
    "    \n",
    "class ShuffleCallback(TrainerCallback):\n",
    "    def on_epoch_begin(self, args, state, control, train_dataloader, **kwargs):\n",
    "        if isinstance(train_dataloader.dataset, IterableDatasetShard):\n",
    "            pass  \n",
    "        elif isinstance(train_dataloader.dataset, IterableDataset):\n",
    "            train_dataloader.dataset.shuffle()       \n",
    "            train_dataloader.dataset.set_epoch(train_dataloader.dataset.epoch + 1)\n",
    "\n",
    "class GradientClippingCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        torch.nn.utils.clip_grad_norm_(kwargs[\"model\"].parameters(), max_norm=1.0)\n",
    "\n",
    "class PredictionLogger(TrainerCallback):\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        if state.global_step % args.logging_steps == 0:\n",
    "            eval_loss = kwargs.get('metrics', {}).get('eval_loss', None)\n",
    "            if eval_loss is not None:\n",
    "                all_predictions = kwargs['metrics']['predictions']\n",
    "                all_labels = kwargs['metrics']['label_ids']\n",
    "                tokenizer = kwargs['tokenizer']\n",
    "\n",
    "                sample_indices = range(min(1, len(all_predictions)))\n",
    "                for idx in sample_indices:\n",
    "                    pred_str = tokenizer.decode(all_predictions[idx], skip_special_tokens=True)\n",
    "                    label_str = tokenizer.decode(all_labels[idx], skip_special_tokens=True)\n",
    "                    print(f\"Evaluation Loss: {eval_loss:.4f}\")\n",
    "                    print(f\"Evaluation Sample {idx}: Prediction: {pred_str}, Label: {label_str}\")\n",
    "\n",
    "class ModelFreezingCallback(TrainerCallback):\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        for name, param in kwargs['model'].named_parameters():\n",
    "            if 'layer.0' in name:  # Change this to match the layers you want to freeze\n",
    "                param.requires_grad = False\n",
    "                print(f\"Freezing layer: {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cer(predictions, label_ids):\n",
    "    pred_str = proccesor.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    label_str = proccesor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    return 100 * cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    wakati = MeCab.Tagger(\"-Owakati\")   \n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = proccesor.tokenizer.pad_token_id\n",
    "    \n",
    "    pred_str = proccesor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = proccesor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    cer = 100 * cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    pred_flat = np.argmax(pred_ids, axis=2).flatten()\n",
    "    labels_flat = label_ids.flatten()\n",
    "    mask = labels_flat != proccesor.tokenizer.pad_token_id\n",
    "\n",
    "    accuracy = accuracy_score(labels_flat[mask], pred_flat[mask])\n",
    "    precision = precision_score(labels_flat[mask], pred_flat[mask], average='weighted')\n",
    "    recall = recall_score(labels_flat[mask], pred_flat[mask], average='weighted')\n",
    "    f1 = f1_score(labels_flat[mask], pred_flat[mask], average='weighted')\n",
    "    \n",
    "    return {\n",
    "        \"cer\": cer,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "tb_writer = SummaryWriter(log_dir=log_dir)\n",
    "tb_callback = CustomTensorBoardCallback(tb_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = \"./output\"\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=log_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=1, \n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2.4e-5,\n",
    "    warmup_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=1000,\n",
    "    tf32=True,\n",
    "    bf16=True,\n",
    "    save_steps=1000,\n",
    "    logging_steps=10,\n",
    "    logging_dir=log_dir+\"logs_trainer/\",\n",
    "    logging_strategy=\"steps\",\n",
    "    report_to=[\"tensorboard\"],\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    hub_private_repo=True,\n",
    "    metric_for_best_model=\"cer\",\n",
    "    predict_with_generate=False,\n",
    "    greater_is_better=False,\n",
    "    generation_max_length=128,\n",
    "    # optim=\"adafactor\",\n",
    "    # weight_decay=0.0025,\n",
    "    disable_tqdm=False,\n",
    "    save_total_limit=2,  \n",
    "    # torch_empty_cache_steps=1,\n",
    ")\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "train = train.shuffle(seed=42)\n",
    "test = test.shuffle(seed=42)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=proccesor.feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, scheduler), # optimizers=(get_adamax_optimizer, None)\n",
    "    callbacks=[CustomTensorBoardCallback(tb_writer), GradientClippingCallback, PredictionLogger],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate()\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "tb_writer.close()\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
